

===== OVERVIEW =====

AI SDK Core: Overview
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Overview
Copy markdown
AI SDK Core
Large Language Models (LLMs) are advanced programs that can understand, create, and engage with human language on a large scale.
They are trained on vast amounts of written material to recognize patterns in language and predict what might come next in a given piece of text.
AI SDK Core
simplifies working with LLMs by offering a standardized way of integrating them into your app
- so you can focus on building great AI applications for your users, not waste time on technical details.
For example, here’s how you can generate text with various models using the AI SDK:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
}
from
"ai"
;
2
3
const
{
text
}
=
await
generateText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
prompt
:
"What is love?"
,
6
}
)
;
Love is a complex and multifaceted emotion that can be felt and expressed in many different ways. It involves deep affection, care, compassion, and connection towards another person or thing.
AI SDK Core Functions
AI SDK Core has various functions designed for
text generation
,
structured data generation
, and
tool usage
.
These functions take a standardized approach to setting up
prompts
and
settings
, making it easier to work with different models.
generateText
: Generates text and
tool calls
.
This function is ideal for non-interactive use cases such as automation tasks where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.
streamText
: Stream text and tool calls.
You can use the
streamText
function for interactive use cases such as
chat bots
and
content streaming
.
generateObject
: Generates a typed, structured object that matches a
Zod
schema.
You can use this function to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.
streamObject
: Stream a structured object that matches a Zod schema.
You can use this function to
stream generated UIs
.
API Reference
Please check out the
AI SDK Core API Reference
for more details on each function.
Previous
AI SDK Core
Next
Generating Text
On this page
AI SDK Core
AI SDK Core Functions
API Reference
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== GENERATING-TEXT =====

AI SDK Core: Generating Text
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Generating Text
Copy markdown
Generating and Streaming Text
Large language models (LLMs) can generate text in response to a prompt, which can contain instructions and information to process.
For example, you can ask a model to come up with a recipe, draft an email, or summarize a document.
The AI SDK Core provides two functions to generate text and stream it from LLMs:
generateText
: Generates text for a given prompt and model.
streamText
: Streams text from a given prompt and model.
Advanced LLM features such as
tool calling
and
structured data generation
are built on top of text generation.
generateText
You can generate text using the
generateText
function. This function is ideal for non-interactive use cases where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
}
from
'ai'
;
2
3
const
{
text
}
=
await
generateText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
prompt
:
'Write a vegetarian lasagna recipe for 4 people.'
,
6
}
)
;
You can use more
advanced prompts
to generate text with more complex instructions and content:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
}
from
'ai'
;
2
3
const
{
text
}
=
await
generateText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
system
:
6
'You are a professional writer. '
+
7
'You write simple, clear, and concise content.'
,
8
prompt
:
`
Summarize the following article in 3-5 sentences:
${
article
}
`
,
9
}
)
;
The result object of
generateText
contains several promises that resolve when all required data is available:
result.content
: The content that was generated in the last step.
result.text
: The generated text.
result.reasoning
: The full reasoning that the model has generated in the last step.
result.reasoningText
: The reasoning text of the model (only available for some models).
result.files
: The files that were generated in the last step.
result.sources
: Sources that have been used as references in the last step (only available for some models).
result.toolCalls
: The tool calls that were made in the last step.
result.toolResults
: The results of the tool calls from the last step.
result.finishReason
: The reason the model finished generating text.
result.rawFinishReason
: The raw reason why the generation finished (from the provider).
result.usage
: The usage of the model during the final step of text generation.
result.totalUsage
: The total usage across all steps (for multi-step generations).
result.warnings
: Warnings from the model provider (e.g. unsupported settings).
result.request
: Additional request information.
result.response
: Additional response information, including response messages and body.
result.providerMetadata
: Additional provider-specific metadata.
result.steps
: Details for all steps, useful for getting information about intermediate steps.
result.output
: The generated structured output using the
output
specification.
Accessing response headers & body
Sometimes you need access to the full response from the model provider,
e.g. to access some provider-specific headers or body content.
You can access the raw response headers and body using the
response
property:
1
import
{
generateText
}
from
'ai'
;
2
3
const
result
=
await
generateText
(
{
4
// ...
5
}
)
;
6
7
console
.
log
(
JSON
.
stringify
(
result
.
response
.
headers
,
null
,
2
)
)
;
8
console
.
log
(
JSON
.
stringify
(
result
.
response
.
body
,
null
,
2
)
)
;
onFinish
callback
When using
generateText
, you can provide an
onFinish
callback that is triggered after the last step is finished (
API Reference
).
It contains the text, usage information, finish reason, messages, steps, total usage, and more:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
}
from
'ai'
;
2
3
const
result
=
await
generateText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
prompt
:
'Invent a new holiday and describe its traditions.'
,
6
onFinish
(
{
text
,
finishReason
,
usage
,
response
,
steps
,
totalUsage
}
)
{
7
// your own logic, e.g. for saving the chat history or recording usage
8
9
const
messages
=
response
.
messages
;
// messages that were generated
10
}
,
11
}
)
;
streamText
Depending on your model and prompt, it can take a large language model (LLM) up to a minute to finish generating its response. This delay can be unacceptable for interactive use cases such as chatbots or real-time applications, where users expect immediate responses.
AI SDK Core provides the
streamText
function which simplifies streaming text from LLMs:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
}
from
'ai'
;
2
3
const
result
=
streamText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
prompt
:
'Invent a new holiday and describe its traditions.'
,
6
}
)
;
7
8
// example: use textStream as an async iterable
9
for
await
(
const
textPart
of
result
.
textStream
)
{
10
console
.
log
(
textPart
)
;
11
}
result.textStream
is both a
ReadableStream
and an
AsyncIterable
.
streamText
immediately starts streaming and suppresses errors to prevent
server crashes. Use the
onError
callback to log errors.
You can use
streamText
on its own or in combination with
AI SDK
UI
and
AI SDK
RSC
.
The result object contains several helper functions to make the integration into
AI SDK UI
easier:
result.toUIMessageStreamResponse()
: Creates a UI Message stream HTTP response (with tool calls etc.) that can be used in a Next.js App Router API route.
result.pipeUIMessageStreamToResponse()
: Writes UI Message stream delta output to a Node.js response-like object.
result.toTextStreamResponse()
: Creates a simple text stream HTTP response.
result.pipeTextStreamToResponse()
: Writes text delta output to a Node.js response-like object.
streamText
is using backpressure and only generates tokens as they are
requested. You need to consume the stream in order for it to finish.
It also provides several promises that resolve when the stream is finished:
result.content
: The content that was generated in the last step.
result.text
: The generated text.
result.reasoning
: The full reasoning that the model has generated.
result.reasoningText
: The reasoning text of the model (only available for some models).
result.files
: Files that have been generated by the model in the last step.
result.sources
: Sources that have been used as references in the last step (only available for some models).
result.toolCalls
: The tool calls that have been executed in the last step.
result.toolResults
: The tool results that have been generated in the last step.
result.finishReason
: The reason the model finished generating text.
result.rawFinishReason
: The raw reason why the generation finished (from the provider).
result.usage
: The usage of the model during the final step of text generation.
result.totalUsage
: The total usage across all steps (for multi-step generations).
result.warnings
: Warnings from the model provider (e.g. unsupported settings).
result.steps
: Details for all steps, useful for getting information about intermediate steps.
result.request
: Additional request information from the last step.
result.response
: Additional response information from the last step.
result.providerMetadata
: Additional provider-specific metadata from the last step.
onError
callback
streamText
immediately starts streaming to enable sending data without waiting for the model.
Errors become part of the stream and are not thrown to prevent e.g. servers from crashing.
To log errors, you can provide an
onError
callback that is triggered when an error occurs.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
}
from
'ai'
;
2
3
const
result
=
streamText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
prompt
:
'Invent a new holiday and describe its traditions.'
,
6
onError
(
{
error
}
)
{
7
console
.
error
(
error
)
;
// your error logging logic here
8
}
,
9
}
)
;
onChunk
callback
When using
streamText
, you can provide an
onChunk
callback that is triggered for each chunk of the stream.
It receives the following chunk types:
text
reasoning
source
tool-call
tool-input-start
tool-input-delta
tool-result
raw
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
}
from
'ai'
;
2
3
const
result
=
streamText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
prompt
:
'Invent a new holiday and describe its traditions.'
,
6
onChunk
(
{
chunk
}
)
{
7
// implement your own logic here, e.g.:
8
if
(
chunk
.
type
===
'text'
)
{
9
console
.
log
(
chunk
.
text
)
;
10
}
11
}
,
12
}
)
;
onFinish
callback
When using
streamText
, you can provide an
onFinish
callback that is triggered when the stream is finished (
API Reference
).
It contains the text, usage information, finish reason, messages, steps, total usage, and more:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
}
from
'ai'
;
2
3
const
result
=
streamText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
prompt
:
'Invent a new holiday and describe its traditions.'
,
6
onFinish
(
{
text
,
finishReason
,
usage
,
response
,
steps
,
totalUsage
}
)
{
7
// your own logic, e.g. for saving the chat history or recording usage
8
9
const
messages
=
response
.
messages
;
// messages that were generated
10
}
,
11
}
)
;
fullStream
property
You can read a stream with all events using the
fullStream
property.
This can be useful if you want to implement your own UI or handle the stream in a different way.
Here is an example of how to use the
fullStream
property:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
result
=
streamText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
tools
:
{
7
cityAttractions
:
{
8
inputSchema
:
z
.
object
(
{
city
:
z
.
string
(
)
}
)
,
9
execute
:
async
(
{
city
}
)
=>
(
{
10
attractions
:
[
'attraction1'
,
'attraction2'
,
'attraction3'
]
,
11
}
)
,
12
}
,
13
}
,
14
prompt
:
'What are some San Francisco tourist attractions?'
,
15
}
)
;
16
17
for
await
(
const
part
of
result
.
fullStream
)
{
18
switch
(
part
.
type
)
{
19
case
'start'
:
{
20
// handle start of stream
21
break
;
22
}
23
case
'start-step'
:
{
24
// handle start of step
25
break
;
26
}
27
case
'text-start'
:
{
28
// handle text start
29
break
;
30
}
31
case
'text-delta'
:
{
32
// handle text delta here
33
break
;
34
}
35
case
'text-end'
:
{
36
// handle text end
37
break
;
38
}
39
case
'reasoning-start'
:
{
40
// handle reasoning start
41
break
;
42
}
43
case
'reasoning-delta'
:
{
44
// handle reasoning delta here
45
break
;
46
}
47
case
'reasoning-end'
:
{
48
// handle reasoning end
49
break
;
50
}
51
case
'source'
:
{
52
// handle source here
53
break
;
54
}
55
case
'file'
:
{
56
// handle file here
57
break
;
58
}
59
case
'tool-call'
:
{
60
switch
(
part
.
toolName
)
{
61
case
'cityAttractions'
:
{
62
// handle tool call here
63
break
;
64
}
65
}
66
break
;
67
}
68
case
'tool-input-start'
:
{
69
// handle tool input start
70
break
;
71
}
72
case
'tool-input-delta'
:
{
73
// handle tool input delta
74
break
;
75
}
76
case
'tool-input-end'
:
{
77
// handle tool input end
78
break
;
79
}
80
case
'tool-result'
:
{
81
switch
(
part
.
toolName
)
{
82
case
'cityAttractions'
:
{
83
// handle tool result here
84
break
;
85
}
86
}
87
break
;
88
}
89
case
'tool-error'
:
{
90
// handle tool error
91
break
;
92
}
93
case
'finish-step'
:
{
94
// handle finish step
95
break
;
96
}
97
case
'finish'
:
{
98
// handle finish here
99
break
;
100
}
101
case
'error'
:
{
102
// handle error here
103
break
;
104
}
105
case
'raw'
:
{
106
// handle raw value
107
break
;
108
}
109
}
110
}
Stream transformation
You can use the
experimental_transform
option to transform the stream.
This is useful for e.g. filtering, changing, or smoothing the text stream.
The transformations are applied before the callbacks are invoked and the promises are resolved.
If you e.g. have a transformation that changes all text to uppercase, the
onFinish
callback will receive the transformed text.
Smoothing streams
The AI SDK Core provides a
smoothStream
function
that
can be used to smooth out text and reasoning streaming.
1
import
{
smoothStream
,
streamText
}
from
'ai'
;
2
3
const
result
=
streamText
(
{
4
model
,
5
prompt
,
6
experimental_transform
:
smoothStream
(
)
,
7
}
)
;
Custom transformations
You can also implement your own custom transformations.
The transformation function receives the tools that are available to the model,
and returns a function that is used to transform the stream.
Tools can either be generic or limited to the tools that you are using.
Here is an example of how to implement a custom transformation that converts
all text to uppercase:
1
const
upperCaseTransform
=
2
<
TOOLS
extends
ToolSet
>
(
)
=>
3
(
options
:
{
tools
:
TOOLS
;
stopStream
:
(
)
=>
void
}
)
=>
4
new
TransformStream
<
TextStreamPart
<
TOOLS
>
,
TextStreamPart
<
TOOLS
>>
(
{
5
transform
(
chunk
,
controller
)
{
6
controller
.
enqueue
(
7
// for text chunks, convert the text to uppercase:
8
chunk
.
type
===
'text'
9
?
{
...
chunk
,
text
:
chunk
.
text
.
toUpperCase
(
)
}
10
:
chunk
,
11
)
;
12
}
,
13
}
)
;
You can also stop the stream using the
stopStream
function.
This is e.g. useful if you want to stop the stream when model guardrails are violated, e.g. by generating inappropriate content.
When you invoke
stopStream
, it is important to simulate the
step-finish
and
finish
events to guarantee that a well-formed stream is returned
and all callbacks are invoked.
1
const
stopWordTransform
=
2
<
TOOLS
extends
ToolSet
>
(
)
=>
3
(
{
stopStream
}
:
{
stopStream
:
(
)
=>
void
}
)
=>
4
new
TransformStream
<
TextStreamPart
<
TOOLS
>
,
TextStreamPart
<
TOOLS
>>
(
{
5
// note: this is a simplified transformation for testing;
6
// in a real-world version more there would need to be
7
// stream buffering and scanning to correctly emit prior text
8
// and to detect all STOP occurrences.
9
transform
(
chunk
,
controller
)
{
10
if
(
chunk
.
type
!==
'text'
)
{
11
controller
.
enqueue
(
chunk
)
;
12
return
;
13
}
14
15
if
(
chunk
.
text
.
includes
(
'STOP'
)
)
{
16
// stop the stream
17
stopStream
(
)
;
18
19
// simulate the finish-step event
20
controller
.
enqueue
(
{
21
type
:
'finish-step'
,
22
finishReason
:
'stop'
,
23
logprobs
:
undefined
,
24
usage
:
{
25
completionTokens
:
NaN
,
26
promptTokens
:
NaN
,
27
totalTokens
:
NaN
,
28
}
,
29
request
:
{
}
,
30
response
:
{
31
id
:
'response-id'
,
32
modelId
:
'mock-model-id'
,
33
timestamp
:
new
Date
(
0
)
,
34
}
,
35
warnings
:
[
]
,
36
isContinued
:
false
,
37
}
)
;
38
39
// simulate the finish event
40
controller
.
enqueue
(
{
41
type
:
'finish'
,
42
finishReason
:
'stop'
,
43
logprobs
:
undefined
,
44
usage
:
{
45
completionTokens
:
NaN
,
46
promptTokens
:
NaN
,
47
totalTokens
:
NaN
,
48
}
,
49
response
:
{
50
id
:
'response-id'
,
51
modelId
:
'mock-model-id'
,
52
timestamp
:
new
Date
(
0
)
,
53
}
,
54
}
)
;
55
56
return
;
57
}
58
59
controller
.
enqueue
(
chunk
)
;
60
}
,
61
}
)
;
Multiple transformations
You can also provide multiple transformations. They are applied in the order they are provided.
1
const
result
=
streamText
(
{
2
model
,
3
prompt
,
4
experimental_transform
:
[
firstTransform
,
secondTransform
]
,
5
}
)
;
Sources
Some providers such as
Perplexity
and
Google Generative AI
include sources in the response.
Currently sources are limited to web pages that ground the response.
You can access them using the
sources
property of the result.
Each
url
source contains the following properties:
id
: The ID of the source.
url
: The URL of the source.
title
: The optional title of the source.
providerMetadata
: Provider metadata for the source.
When you use
generateText
, you can access the sources using the
sources
property:
1
const
result
=
await
generateText
(
{
2
model
:
'google/gemini-2.5-flash'
,
3
tools
:
{
4
google_search
:
google
.
tools
.
googleSearch
(
{
}
)
,
5
}
,
6
prompt
:
'List the top 5 San Francisco news from the past week.'
,
7
}
)
;
8
9
for
(
const
source
of
result
.
sources
)
{
10
if
(
source
.
sourceType
===
'url'
)
{
11
console
.
log
(
'ID:'
,
source
.
id
)
;
12
console
.
log
(
'Title:'
,
source
.
title
)
;
13
console
.
log
(
'URL:'
,
source
.
url
)
;
14
console
.
log
(
'Provider metadata:'
,
source
.
providerMetadata
)
;
15
console
.
log
(
)
;
16
}
17
}
When you use
streamText
, you can access the sources using the
fullStream
property:
1
const
result
=
streamText
(
{
2
model
:
'google/gemini-2.5-flash'
,
3
tools
:
{
4
google_search
:
google
.
tools
.
googleSearch
(
{
}
)
,
5
}
,
6
prompt
:
'List the top 5 San Francisco news from the past week.'
,
7
}
)
;
8
9
for
await
(
const
part
of
result
.
fullStream
)
{
10
if
(
part
.
type
===
'source'
&&
part
.
sourceType
===
'url'
)
{
11
console
.
log
(
'ID:'
,
part
.
id
)
;
12
console
.
log
(
'Title:'
,
part
.
title
)
;
13
console
.
log
(
'URL:'
,
part
.
url
)
;
14
console
.
log
(
'Provider metadata:'
,
part
.
providerMetadata
)
;
15
console
.
log
(
)
;
16
}
17
}
The sources are also available in the
result.sources
promise.
Examples
You can see
generateText
and
streamText
in action using various frameworks in the following examples:
generateText
Learn to generate text in Node.js
Learn to generate text in Next.js with Route Handlers (AI SDK UI)
Learn to generate text in Next.js with Server Actions (AI SDK RSC)
streamText
Learn to stream text in Node.js
Learn to stream text in Next.js with Route Handlers (AI SDK UI)
Learn to stream text in Next.js with Server Actions (AI SDK RSC)
Previous
Overview
Next
Generating Structured Data
On this page
Generating and Streaming Text
generateText
Accessing response headers & body
onFinish callback
streamText
onError callback
onChunk callback
onFinish callback
fullStream property
Stream transformation
Smoothing streams
Custom transformations
Multiple transformations
Sources
Examples
generateText
streamText
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== GENERATING-STRUCTURED-DATA =====

AI SDK Core: Generating Structured Data
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Generating Structured Data
Copy markdown
Generating Structured Data
While text generation can be useful, your use case will likely call for generating structured data.
For example, you might want to extract information from text, classify data, or generate synthetic data.
Many language models are capable of generating structured data, often defined as using "JSON modes" or "tools".
However, you need to manually provide schemas and then validate the generated data as LLMs can produce incorrect or incomplete structured data.
The AI SDK standardises structured object generation across model providers
using the
output
property on
generateText
and
streamText
.
You can use
Zod schemas
,
Valibot
, or
JSON schemas
to specify the shape of the data that you want,
and the AI model will generate data that conforms to that structure.
Structured output generation is part of the
generateText
and
streamText
flow. This means you can combine it with tool calling in the same request.
Generating Structured Outputs
Use
generateText
with
Output.object()
to generate structured data from a prompt.
The schema is also used to validate the generated data, ensuring type safety and correctness.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
,
Output
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
{
output
}
=
await
generateText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
output
:
Output
.
object
(
{
7
schema
:
z
.
object
(
{
8
recipe
:
z
.
object
(
{
9
name
:
z
.
string
(
)
,
10
ingredients
:
z
.
array
(
11
z
.
object
(
{
name
:
z
.
string
(
)
,
amount
:
z
.
string
(
)
}
)
,
12
)
,
13
steps
:
z
.
array
(
z
.
string
(
)
)
,
14
}
)
,
15
}
)
,
16
}
)
,
17
prompt
:
'Generate a lasagna recipe.'
,
18
}
)
;
Structured output generation counts as a step in the AI SDK's multi-turn
execution model (where each model call or tool execution is one step). When
combining with tools, account for this in your
stopWhen
configuration.
Accessing response headers & body
Sometimes you need access to the full response from the model provider,
e.g. to access some provider-specific headers or body content.
You can access the raw response headers and body using the
response
property:
1
import
{
generateText
,
Output
}
from
'ai'
;
2
3
const
result
=
await
generateText
(
{
4
// ...
5
output
:
Output
.
object
(
{
schema
}
)
,
6
}
)
;
7
8
console
.
log
(
JSON
.
stringify
(
result
.
response
.
headers
,
null
,
2
)
)
;
9
console
.
log
(
JSON
.
stringify
(
result
.
response
.
body
,
null
,
2
)
)
;
Stream Structured Outputs
Given the added complexity of returning structured data, model response time can be unacceptable for your interactive use case.
With
streamText
and
output
, you can stream the model's structured response as it is generated.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
,
Output
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
{
partialOutputStream
}
=
streamText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
output
:
Output
.
object
(
{
7
schema
:
z
.
object
(
{
8
recipe
:
z
.
object
(
{
9
name
:
z
.
string
(
)
,
10
ingredients
:
z
.
array
(
11
z
.
object
(
{
name
:
z
.
string
(
)
,
amount
:
z
.
string
(
)
}
)
,
12
)
,
13
steps
:
z
.
array
(
z
.
string
(
)
)
,
14
}
)
,
15
}
)
,
16
}
)
,
17
prompt
:
'Generate a lasagna recipe.'
,
18
}
)
;
19
20
// use partialOutputStream as an async iterable
21
for
await
(
const
partialObject
of
partialOutputStream
)
{
22
console
.
log
(
partialObject
)
;
23
}
You can consume the structured output on the client with the
useObject
hook.
Error Handling in Streams
streamText
starts streaming immediately. When errors occur during streaming, they become part of the stream rather than thrown exceptions (to prevent stream crashes).
To handle errors, provide an
onError
callback:
1
import
{
streamText
,
Output
}
from
'ai'
;
2
3
const
result
=
streamText
(
{
4
// ...
5
output
:
Output
.
object
(
{
schema
}
)
,
6
onError
(
{
error
}
)
{
7
console
.
error
(
error
)
;
// log to your error tracking service
8
}
,
9
}
)
;
For non-streaming error handling with
generateText
, see the
Error Handling
section below.
Output Types
The AI SDK supports multiple ways of specifying the expected structure of generated data via the
Output
object. You can select from various strategies for structured/text generation and validation.
Output.text()
Use
Output.text()
to generate plain text from a model. This option doesn't enforce any schema on the result: you simply receive the model's text as a string. This is the default behavior when no
output
is specified.
1
import
{
generateText
,
Output
}
from
'ai'
;
2
3
const
{
output
}
=
await
generateText
(
{
4
// ...
5
output
:
Output
.
text
(
)
,
6
prompt
:
'Tell me a joke.'
,
7
}
)
;
8
// output will be a string (the joke)
Output.object()
Use
Output.object({ schema })
to generate a structured object based on a schema (for example, a Zod schema). The output is type-validated to ensure the returned result matches the schema.
1
import
{
generateText
,
Output
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
{
output
}
=
await
generateText
(
{
5
// ...
6
output
:
Output
.
object
(
{
7
schema
:
z
.
object
(
{
8
name
:
z
.
string
(
)
,
9
age
:
z
.
number
(
)
.
nullable
(
)
,
10
labels
:
z
.
array
(
z
.
string
(
)
)
,
11
}
)
,
12
}
)
,
13
prompt
:
'Generate information for a test user.'
,
14
}
)
;
15
// output will be an object matching the schema above
Partial outputs streamed via
streamText
cannot be validated against your
provided schema, as incomplete data may not yet conform to the expected
structure.
Output.array()
Use
Output.array({ element })
to specify that you expect an array of typed objects from the model, where each element should conform to a schema (defined in the
element
property).
1
import
{
generateText
,
Output
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
{
output
}
=
await
generateText
(
{
5
// ...
6
output
:
Output
.
array
(
{
7
element
:
z
.
object
(
{
8
location
:
z
.
string
(
)
,
9
temperature
:
z
.
number
(
)
,
10
condition
:
z
.
string
(
)
,
11
}
)
,
12
}
)
,
13
prompt
:
'List the weather for San Francisco and Paris.'
,
14
}
)
;
15
// output will be an array of objects like:
16
// [
17
//   { location: 'San Francisco', temperature: 70, condition: 'Sunny' },
18
//   { location: 'Paris', temperature: 65, condition: 'Cloudy' },
19
// ]
Output.choice()
Use
Output.choice({ options })
when you expect the model to choose from a specific set of string options, such as for classification or fixed-enum answers.
1
import
{
generateText
,
Output
}
from
'ai'
;
2
3
const
{
output
}
=
await
generateText
(
{
4
// ...
5
output
:
Output
.
choice
(
{
6
options
:
[
'sunny'
,
'rainy'
,
'snowy'
]
,
7
}
)
,
8
prompt
:
'Is the weather sunny, rainy, or snowy today?'
,
9
}
)
;
10
// output will be one of: 'sunny', 'rainy', or 'snowy'
You can provide any set of string options, and the output will always be a single string value that matches one of the specified options. The AI SDK validates that the result matches one of your options, and will throw if the model returns something invalid.
This is especially useful for making classification-style generations or forcing valid values for API compatibility.
Output.json()
Use
Output.json()
when you want to generate and parse unstructured JSON values from the model, without enforcing a specific schema. This is useful if you want to capture arbitrary objects, flexible structures, or when you want to rely on the model's natural output rather than rigid validation.
1
import
{
generateText
,
Output
}
from
'ai'
;
2
3
const
{
output
}
=
await
generateText
(
{
4
// ...
5
output
:
Output
.
json
(
)
,
6
prompt
:
7
'For each city, return the current temperature and weather condition as a JSON object.'
,
8
}
)
;
9
10
// output could be any valid JSON, for example:
11
// {
12
//   "San Francisco": { "temperature": 70, "condition": "Sunny" },
13
//   "Paris": { "temperature": 65, "condition": "Cloudy" }
14
// }
With
Output.json
, the AI SDK only checks that the response is valid JSON; it doesn't validate the structure or types of the values. If you need schema validation, use the
.object
or
.array
outputs instead.
For more advanced validation or different structures, see
the Output API reference
.
Generating Structured Outputs with Tools
One of the key advantages of using structured output with
generateText
and
streamText
is the ability to combine it with tool calling.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
,
Output
,
tool
,
stepCountIs
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
{
output
}
=
await
generateText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
tools
:
{
7
weather
:
tool
(
{
8
description
:
'Get the weather for a location'
,
9
inputSchema
:
z
.
object
(
{
location
:
z
.
string
(
)
}
)
,
10
execute
:
async
(
{
location
}
)
=>
{
11
// fetch weather data
12
return
{
temperature
:
72
,
condition
:
'sunny'
}
;
13
}
,
14
}
)
,
15
}
,
16
output
:
Output
.
object
(
{
17
schema
:
z
.
object
(
{
18
summary
:
z
.
string
(
)
,
19
recommendation
:
z
.
string
(
)
,
20
}
)
,
21
}
)
,
22
stopWhen
:
stepCountIs
(
5
)
,
23
prompt
:
'What should I wear in San Francisco today?'
,
24
}
)
;
When using tools with structured output, remember that generating the
structured output counts as a step. Configure
stopWhen
to allow enough steps
for both tool execution and output generation.
Property Descriptions
You can add
.describe("...")
to individual schema properties to give the model hints about what each property is for. This helps improve the quality and accuracy of generated structured data:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
,
Output
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
{
output
}
=
await
generateText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
output
:
Output
.
object
(
{
7
schema
:
z
.
object
(
{
8
name
:
z
.
string
(
)
.
describe
(
'The name of the recipe'
)
,
9
ingredients
:
z
10
.
array
(
11
z
.
object
(
{
12
name
:
z
.
string
(
)
,
13
amount
:
z
14
.
string
(
)
15
.
describe
(
'The amount of the ingredient (grams or ml)'
)
,
16
}
)
,
17
)
18
.
describe
(
'List of ingredients with amounts'
)
,
19
steps
:
z
.
array
(
z
.
string
(
)
)
.
describe
(
'Step-by-step cooking instructions'
)
,
20
}
)
,
21
}
)
,
22
prompt
:
'Generate a lasagna recipe.'
,
23
}
)
;
Property descriptions are particularly useful for:
Clarifying ambiguous property names
Specifying expected formats or conventions
Providing context for complex nested structures
Output Name and Description
You can optionally specify a
name
and
description
for the output. These are used by some providers for additional LLM guidance, e.g. via tool or schema name.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
,
Output
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
{
output
}
=
await
generateText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
output
:
Output
.
object
(
{
7
name
:
'Recipe'
,
8
description
:
'A recipe for a dish.'
,
9
schema
:
z
.
object
(
{
10
name
:
z
.
string
(
)
,
11
ingredients
:
z
.
array
(
z
.
object
(
{
name
:
z
.
string
(
)
,
amount
:
z
.
string
(
)
}
)
)
,
12
steps
:
z
.
array
(
z
.
string
(
)
)
,
13
}
)
,
14
}
)
,
15
prompt
:
'Generate a lasagna recipe.'
,
16
}
)
;
This works with all output types that support structured generation:
Output.object({ name, description, schema })
Output.array({ name, description, element })
Output.choice({ name, description, options })
Output.json({ name, description })
Accessing Reasoning
You can access the reasoning used by the language model to generate the object via the
reasoning
property on the result. This property contains a string with the model's thought process, if available.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
,
Output
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
result
=
await
generateText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
// must be a reasoning model
6
output
:
Output
.
object
(
{
7
schema
:
z
.
object
(
{
8
recipe
:
z
.
object
(
{
9
name
:
z
.
string
(
)
,
10
ingredients
:
z
.
array
(
11
z
.
object
(
{
12
name
:
z
.
string
(
)
,
13
amount
:
z
.
string
(
)
,
14
}
)
,
15
)
,
16
steps
:
z
.
array
(
z
.
string
(
)
)
,
17
}
)
,
18
}
)
,
19
}
)
,
20
prompt
:
'Generate a lasagna recipe.'
,
21
}
)
;
22
23
console
.
log
(
result
.
reasoning
)
;
Error Handling
When
generateText
with structured output cannot generate a valid object, it throws a
AI_NoObjectGeneratedError
.
This error occurs when the AI provider fails to generate a parsable object that conforms to the schema.
It can arise due to the following reasons:
The model failed to generate a response.
The model generated a response that could not be parsed.
The model generated a response that could not be validated against the schema.
The error preserves the following information to help you log the issue:
text
: The text that was generated by the model. This can be the raw text or the tool call text, depending on the object generation mode.
response
: Metadata about the language model response, including response id, timestamp, and model.
usage
: Request token usage.
cause
: The cause of the error (e.g. a JSON parsing error). You can use this for more detailed error handling.
1
import
{
generateText
,
Output
,
NoObjectGeneratedError
}
from
'ai'
;
2
3
try
{
4
await
generateText
(
{
5
model
,
6
output
:
Output
.
object
(
{
schema
}
)
,
7
prompt
,
8
}
)
;
9
}
catch
(
error
)
{
10
if
(
NoObjectGeneratedError
.
isInstance
(
error
)
)
{
11
console
.
log
(
'NoObjectGeneratedError'
)
;
12
console
.
log
(
'Cause:'
,
error
.
cause
)
;
13
console
.
log
(
'Text:'
,
error
.
text
)
;
14
console
.
log
(
'Response:'
,
error
.
response
)
;
15
console
.
log
(
'Usage:'
,
error
.
usage
)
;
16
}
17
}
generateObject and streamObject (Legacy)
generateObject
and
streamObject
are deprecated. Use
generateText
and
streamText
with the
output
property instead. The legacy functions will be
removed in a future major version.
The
generateObject
and
streamObject
functions are the legacy way to generate structured data. They work similarly to
generateText
and
streamText
with
Output.object()
, but as standalone functions.
generateObject
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateObject
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
{
object
}
=
await
generateObject
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
schema
:
z
.
object
(
{
7
recipe
:
z
.
object
(
{
8
name
:
z
.
string
(
)
,
9
ingredients
:
z
.
array
(
z
.
object
(
{
name
:
z
.
string
(
)
,
amount
:
z
.
string
(
)
}
)
)
,
10
steps
:
z
.
array
(
z
.
string
(
)
)
,
11
}
)
,
12
}
)
,
13
prompt
:
'Generate a lasagna recipe.'
,
14
}
)
;
streamObject
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamObject
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
{
partialObjectStream
}
=
streamObject
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
schema
:
z
.
object
(
{
7
recipe
:
z
.
object
(
{
8
name
:
z
.
string
(
)
,
9
ingredients
:
z
.
array
(
z
.
object
(
{
name
:
z
.
string
(
)
,
amount
:
z
.
string
(
)
}
)
)
,
10
steps
:
z
.
array
(
z
.
string
(
)
)
,
11
}
)
,
12
}
)
,
13
prompt
:
'Generate a lasagna recipe.'
,
14
}
)
;
15
16
for
await
(
const
partialObject
of
partialObjectStream
)
{
17
console
.
log
(
partialObject
)
;
18
}
Schema Name and Description (Legacy)
You can optionally specify a name and description for the schema. These are used by some providers for additional LLM guidance, e.g. via tool or schema name.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateObject
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
{
object
}
=
await
generateObject
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
schemaName
:
'Recipe'
,
7
schemaDescription
:
'A recipe for a dish.'
,
8
schema
:
z
.
object
(
{
9
name
:
z
.
string
(
)
,
10
ingredients
:
z
.
array
(
z
.
object
(
{
name
:
z
.
string
(
)
,
amount
:
z
.
string
(
)
}
)
)
,
11
steps
:
z
.
array
(
z
.
string
(
)
)
,
12
}
)
,
13
prompt
:
'Generate a lasagna recipe.'
,
14
}
)
;
Output Strategy (Legacy)
The legacy functions support different output strategies via the
output
parameter:
Array
Generate an array of objects. The schema specifies the shape of an array element.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamObject
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
{
elementStream
}
=
streamObject
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
output
:
'array'
,
7
schema
:
z
.
object
(
{
8
name
:
z
.
string
(
)
,
9
class
:
z
10
.
string
(
)
11
.
describe
(
'Character class, e.g. warrior, mage, or thief.'
)
,
12
description
:
z
.
string
(
)
,
13
}
)
,
14
prompt
:
'Generate 3 hero descriptions for a fantasy role playing game.'
,
15
}
)
;
16
17
for
await
(
const
hero
of
elementStream
)
{
18
console
.
log
(
hero
)
;
19
}
Enum
Generate a specific enum value for classification tasks.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateObject
}
from
'ai'
;
2
3
const
{
object
}
=
await
generateObject
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
output
:
'enum'
,
6
enum
:
[
'action'
,
'comedy'
,
'drama'
,
'horror'
,
'sci-fi'
]
,
7
prompt
:
8
'Classify the genre of this movie plot: '
+
9
'"A group of astronauts travel through a wormhole in search of a '
+
10
'new habitable planet for humanity."'
,
11
}
)
;
No Schema
Generate unstructured JSON without a schema.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateObject
}
from
'ai'
;
2
3
const
{
object
}
=
await
generateObject
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
output
:
'no-schema'
,
6
prompt
:
'Generate a lasagna recipe.'
,
7
}
)
;
Repairing Invalid JSON (Legacy)
The
repairText
function is experimental and may change in the future.
Sometimes the model will generate invalid or malformed JSON.
You can use the
repairText
function to attempt to repair the JSON.
1
import
{
generateObject
}
from
'ai'
;
2
3
const
{
object
}
=
await
generateObject
(
{
4
model
,
5
schema
,
6
prompt
,
7
experimental_repairText
:
async
(
{
text
,
error
}
)
=>
{
8
// example: add a closing brace to the text
9
return
text
+
'}'
;
10
}
,
11
}
)
;
More Examples
You can see
generateObject
and
streamObject
in action using various frameworks in the following examples:
generateObject
Learn to generate objects in Node.js
Learn to generate objects in Next.js with Route Handlers (AI SDK UI)
Learn to generate objects in Next.js with Server Actions (AI SDK RSC)
streamText
with Output
Learn to stream objects in Node.js
Learn to stream objects in Next.js with Route Handlers (AI SDK UI)
Learn to stream objects in Next.js with Server Actions (AI SDK RSC)
Previous
Generating Text
Next
Tool Calling
On this page
Generating Structured Data
Generating Structured Outputs
Accessing response headers & body
Stream Structured Outputs
Error Handling in Streams
Output Types
Output.text()
Output.object()
Output.array()
Output.choice()
Output.json()
Generating Structured Outputs with Tools
Property Descriptions
Output Name and Description
Accessing Reasoning
Error Handling
generateObject and streamObject (Legacy)
generateObject
streamObject
Schema Name and Description (Legacy)
Output Strategy (Legacy)
Array
Enum
No Schema
Repairing Invalid JSON (Legacy)
More Examples
generateObject
streamText with Output
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== TOOLS-AND-TOOL-CALLING =====

AI SDK Core: Tool Calling
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Tool Calling
Copy markdown
Tool Calling
As covered under Foundations,
tools
are objects that can be called by the model to perform a specific task.
AI SDK Core tools contain several core elements:
description
: An optional description of the tool that can influence when the tool is picked.
inputSchema
: A
Zod schema
or a
JSON schema
that defines the input parameters. The schema is consumed by the LLM, and also used to validate the LLM tool calls.
execute
: An optional async function that is called with the inputs from the tool call. It produces a value of type
RESULT
(generic type). It is optional because you might want to forward tool calls to the client or to a queue instead of executing them in the same process.
strict
:
(optional, boolean)
Enables strict tool calling when supported by the provider
You can use the
tool
helper function to
infer the types of the
execute
parameters.
The
tools
parameter of
generateText
and
streamText
is an object that has the tool names as keys and the tools as values:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
z
}
from
'zod'
;
2
import
{
generateText
,
tool
,
stopWhen
}
from
'ai'
;
3
4
const
result
=
await
generateText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
tools
:
{
7
weather
:
tool
(
{
8
description
:
'Get the weather in a location'
,
9
inputSchema
:
z
.
object
(
{
10
location
:
z
.
string
(
)
.
describe
(
'The location to get the weather for'
)
,
11
}
)
,
12
execute
:
async
(
{
location
}
)
=>
(
{
13
location
,
14
temperature
:
72
+
Math
.
floor
(
Math
.
random
(
)
*
21
)
-
10
,
15
}
)
,
16
}
)
,
17
}
,
18
stopWhen
:
stepCountIs
(
5
)
,
19
prompt
:
'What is the weather in San Francisco?'
,
20
}
)
;
When a model uses a tool, it is called a "tool call" and the output of the
tool is called a "tool result".
Tool calling is not restricted to only text generation.
You can also use it to render user interfaces (Generative UI).
Strict Mode
When enabled, language model providers that support strict tool calling will only generate tool calls that are valid according to your defined
inputSchema
.
This increases the reliability of tool calling.
However, not all schemas may be supported in strict mode, and what is supported depends on the specific provider.
By default, strict mode is disabled. You can enable it per-tool by setting
strict: true
:
1
tool
(
{
2
description
:
'Get the weather in a location'
,
3
inputSchema
:
z
.
object
(
{
4
location
:
z
.
string
(
)
,
5
}
)
,
6
strict
:
true
,
// Enable strict validation for this tool
7
execute
:
async
(
{
location
}
)
=>
(
{
8
// ...
9
}
)
,
10
}
)
;
Not all providers or models support strict mode. For those that do not, this
option is ignored.
Input Examples
You can specify example inputs for your tools to help guide the model on how input data should be structured.
When supported by providers, input examples can help when JSON schema itself does not fully specify the intended
usage or when there are optional values.
1
tool
(
{
2
description
:
'Get the weather in a location'
,
3
inputSchema
:
z
.
object
(
{
4
location
:
z
.
string
(
)
.
describe
(
'The location to get the weather for'
)
,
5
}
)
,
6
inputExamples
:
[
7
{
input
:
{
location
:
'San Francisco'
}
}
,
8
{
input
:
{
location
:
'London'
}
}
,
9
]
,
10
execute
:
async
(
{
location
}
)
=>
{
11
// ...
12
}
,
13
}
)
;
Only the Anthropic providers supports tool input examples natively. Other
providers ignore the setting.
Tool Execution Approval
By default, tools with an
execute
function run automatically as the model calls them. You can require approval before execution by setting
needsApproval
:
1
import
{
tool
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
runCommand
=
tool
(
{
5
description
:
'Run a shell command'
,
6
inputSchema
:
z
.
object
(
{
7
command
:
z
.
string
(
)
.
describe
(
'The shell command to execute'
)
,
8
}
)
,
9
needsApproval
:
true
,
10
execute
:
async
(
{
command
}
)
=>
{
11
// your command execution logic here
12
}
,
13
}
)
;
This is useful for tools that perform sensitive operations like executing commands, processing payments, modifying data, and more potentially dangerous actions.
How It Works
When a tool requires approval,
generateText
and
streamText
don't pause execution. Instead, they complete and return
tool-approval-request
parts in the result content. This means the approval flow requires two calls to the model: the first returns the approval request, and the second (after receiving the approval response) either executes the tool or informs the model that approval was denied.
Here's the complete flow:
Call
generateText
with a tool that has
needsApproval: true
Model generates a tool call
generateText
returns with
tool-approval-request
parts in
result.content
Your app requests an approval and collects the user's decision
Add a
tool-approval-response
to the messages array
Call
generateText
again with the updated messages
If approved, the tool runs and returns a result. If denied, the model sees the denial and responds accordingly.
Handling Approval Requests
After calling
generateText
or
streamText
, check
result.content
for
tool-approval-request
parts:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
type
ModelMessage
,
generateText
}
from
'ai'
;
2
3
const
messages
:
ModelMessage
[
]
=
[
4
{
role
:
'user'
,
content
:
'Remove the most recent file'
}
,
5
]
;
6
const
result
=
await
generateText
(
{
7
model
:
"anthropic/claude-sonnet-4.5"
,
8
tools
:
{
runCommand
}
,
9
messages
,
10
}
)
;
11
12
messages
.
push
(
...
result
.
response
.
messages
)
;
13
14
for
(
const
part
of
result
.
content
)
{
15
if
(
part
.
type
===
'tool-approval-request'
)
{
16
console
.
log
(
part
.
approvalId
)
;
// Unique ID for this approval request
17
console
.
log
(
part
.
toolCall
)
;
// Contains toolName, input, etc.
18
}
19
}
To respond, create a
tool-approval-response
and add it to your messages:
1
import
{
type
ToolApprovalResponse
}
from
'ai'
;
2
3
const
approvals
:
ToolApprovalResponse
[
]
=
[
]
;
4
5
for
(
const
part
of
result
.
content
)
{
6
if
(
part
.
type
===
'tool-approval-request'
)
{
7
const
response
:
ToolApprovalResponse
=
{
8
type
:
'tool-approval-response'
,
9
approvalId
:
part
.
approvalId
,
10
approved
:
true
,
// or false to deny
11
reason
:
'User confirmed the command'
,
// Optional context for the model
12
}
;
13
approvals
.
push
(
response
)
;
14
}
15
}
16
17
// add approvals to messages
18
messages
.
push
(
{
role
:
'tool'
,
content
:
approvals
}
)
;
Then call
generateText
again with the updated messages. If approved, the tool executes. If denied, the model receives the denial and can respond accordingly.
When a tool execution is denied, consider adding a system instruction like
"When a tool execution is not approved, do not retry it" to prevent the model
from attempting the same call again.
Dynamic Approval
You can make approval decisions based on tool input by providing an async function:
1
const
paymentTool
=
tool
(
{
2
description
:
'Process a payment'
,
3
inputSchema
:
z
.
object
(
{
4
amount
:
z
.
number
(
)
,
5
recipient
:
z
.
string
(
)
,
6
}
)
,
7
needsApproval
:
async
(
{
amount
}
)
=>
amount
>
1000
,
8
execute
:
async
(
{
amount
,
recipient
}
)
=>
{
9
return
await
processPayment
(
amount
,
recipient
)
;
10
}
,
11
}
)
;
In this example, only transactions over $1000 require approval. Smaller transactions execute automatically.
Tool Execution Approval with useChat
When using
useChat
, the approval flow is handled through UI state. See
Chatbot Tool Usage
for details on handling approvals in your UI with
addToolApprovalResponse
.
Multi-Step Calls (using stopWhen)
With the
stopWhen
setting, you can enable multi-step calls in
generateText
and
streamText
. When
stopWhen
is set and the model generates a tool call, the AI SDK will trigger a new generation passing in the tool result until there are no further tool calls or the stopping condition is met.
The
stopWhen
conditions are only evaluated when the last step contains tool
results.
By default, when you use
generateText
or
streamText
, it triggers a single generation. This works well for many use cases where you can rely on the model's training data to generate a response. However, when you provide tools, the model now has the choice to either generate a normal text response, or generate a tool call. If the model generates a tool call, its generation is complete and that step is finished.
You may want the model to generate text after the tool has been executed, either to summarize the tool results in the context of the users query. In many cases, you may also want the model to use multiple tools in a single response. This is where multi-step calls come in.
You can think of multi-step calls in a similar way to a conversation with a human. When you ask a question, if the person does not have the requisite knowledge in their common knowledge (a model's training data), the person may need to look up information (use a tool) before they can provide you with an answer. In the same way, the model may need to call a tool to get the information it needs to answer your question where each generation (tool call or text generation) is a step.
Example
In the following example, there are two steps:
Step 1
The prompt
'What is the weather in San Francisco?'
is sent to the model.
The model generates a tool call.
The tool call is executed.
Step 2
The tool result is sent to the model.
The model generates a response considering the tool result.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
z
}
from
'zod'
;
2
import
{
generateText
,
tool
,
stepCountIs
}
from
'ai'
;
3
4
const
{
text
,
steps
}
=
await
generateText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
tools
:
{
7
weather
:
tool
(
{
8
description
:
'Get the weather in a location'
,
9
inputSchema
:
z
.
object
(
{
10
location
:
z
.
string
(
)
.
describe
(
'The location to get the weather for'
)
,
11
}
)
,
12
execute
:
async
(
{
location
}
)
=>
(
{
13
location
,
14
temperature
:
72
+
Math
.
floor
(
Math
.
random
(
)
*
21
)
-
10
,
15
}
)
,
16
}
)
,
17
}
,
18
stopWhen
:
stepCountIs
(
5
)
,
// stop after a maximum of 5 steps if tools were called
19
prompt
:
'What is the weather in San Francisco?'
,
20
}
)
;
You can use
streamText
in a similar way.
Steps
To access intermediate tool calls and results, you can use the
steps
property in the result object
or the
streamText
onFinish
callback.
It contains all the text, tool calls, tool results, and more from each step.
Example: Extract tool results from all steps
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
}
from
'ai'
;
2
3
const
{
steps
}
=
await
generateText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
stopWhen
:
stepCountIs
(
10
)
,
6
// ...
7
}
)
;
8
9
// extract all tool calls from the steps:
10
const
allToolCalls
=
steps
.
flatMap
(
step
=>
step
.
toolCalls
)
;
onStepFinish
callback
When using
generateText
or
streamText
, you can provide an
onStepFinish
callback that
is triggered when a step is finished,
i.e. all text deltas, tool calls, and tool results for the step are available.
When you have multiple steps, the callback is triggered for each step.
1
import
{
generateText
}
from
'ai'
;
2
3
const
result
=
await
generateText
(
{
4
// ...
5
onStepFinish
(
{
text
,
toolCalls
,
toolResults
,
finishReason
,
usage
}
)
{
6
// your own logic, e.g. for saving the chat history or recording usage
7
}
,
8
}
)
;
prepareStep
callback
The
prepareStep
callback is called before a step is started.
It is called with the following parameters:
model
: The model that was passed into
generateText
.
stopWhen
: The stopping condition that was passed into
generateText
.
stepNumber
: The number of the step that is being executed.
steps
: The steps that have been executed so far.
messages
: The messages that will be sent to the model for the current step.
experimental_context
: The context passed via the
experimental_context
setting (experimental).
You can use it to provide different settings for a step, including modifying the input messages.
1
import
{
generateText
}
from
'ai'
;
2
3
const
result
=
await
generateText
(
{
4
// ...
5
prepareStep
:
async
(
{
model
,
stepNumber
,
steps
,
messages
}
)
=>
{
6
if
(
stepNumber
===
0
)
{
7
return
{
8
// use a different model for this step:
9
model
:
modelForThisParticularStep
,
10
// force a tool choice for this step:
11
toolChoice
:
{
type
:
'tool'
,
toolName
:
'tool1'
}
,
12
// limit the tools that are available for this step:
13
activeTools
:
[
'tool1'
]
,
14
}
;
15
}
16
17
// when nothing is returned, the default settings are used
18
}
,
19
}
)
;
Message Modification for Longer Agentic Loops
In longer agentic loops, you can use the
messages
parameter to modify the input messages for each step. This is particularly useful for prompt compression:
1
prepareStep
:
async
(
{
stepNumber
,
steps
,
messages
}
)
=>
{
2
// Compress conversation history for longer loops
3
if
(
messages
.
length
>
20
)
{
4
return
{
5
messages
:
messages
.
slice
(
-
10
)
,
6
}
;
7
}
8
9
return
{
}
;
10
}
,
Provider Options for Step Configuration
You can use
providerOptions
in
prepareStep
to pass provider-specific configuration for each step. This is useful for features like Anthropic's code execution container persistence:
1
import
{
forwardAnthropicContainerIdFromLastStep
}
from
'@ai-sdk/anthropic'
;
2
3
// Propagate container ID from previous step for code execution continuity
4
prepareStep
:
forwardAnthropicContainerIdFromLastStep
,
Response Messages
Adding the generated assistant and tool messages to your conversation history is a common task,
especially if you are using multi-step tool calls.
Both
generateText
and
streamText
have a
response.messages
property that you can use to
add the assistant and tool messages to your conversation history.
It is also available in the
onFinish
callback of
streamText
.
The
response.messages
property contains an array of
ModelMessage
objects that you can add to your conversation history:
1
import
{
generateText
,
ModelMessage
}
from
'ai'
;
2
3
const
messages
:
ModelMessage
[
]
=
[
4
// ...
5
]
;
6
7
const
{
response
}
=
await
generateText
(
{
8
// ...
9
messages
,
10
}
)
;
11
12
// add the response messages to your conversation history:
13
messages
.
push
(
...
response
.
messages
)
;
// streamText: ...((await response).messages)
Dynamic Tools
AI SDK Core supports dynamic tools for scenarios where tool schemas are not known at compile time. This is useful for:
MCP (Model Context Protocol) tools without schemas
User-defined functions at runtime
Tools loaded from external sources
Using dynamicTool
The
dynamicTool
helper creates tools with unknown input/output types:
1
import
{
dynamicTool
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
customTool
=
dynamicTool
(
{
5
description
:
'Execute a custom function'
,
6
inputSchema
:
z
.
object
(
{
}
)
,
7
execute
:
async
input
=>
{
8
// input is typed as 'unknown'
9
// You need to validate/cast it at runtime
10
const
{
action
,
parameters
}
=
input
as
any
;
11
12
// Execute your dynamic logic
13
return
{
result
:
`
Executed
${
action
}
`
}
;
14
}
,
15
}
)
;
Type-Safe Handling
When using both static and dynamic tools, use the
dynamic
flag for type narrowing:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateText
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
tools
:
{
4
// Static tool with known types
5
weather
:
weatherTool
,
6
// Dynamic tool
7
custom
:
dynamicTool
(
{
8
/* ... */
9
}
)
,
10
}
,
11
onStepFinish
:
(
{
toolCalls
,
toolResults
}
)
=>
{
12
// Type-safe iteration
13
for
(
const
toolCall
of
toolCalls
)
{
14
if
(
toolCall
.
dynamic
)
{
15
// Dynamic tool: input is 'unknown'
16
console
.
log
(
'Dynamic:'
,
toolCall
.
toolName
,
toolCall
.
input
)
;
17
continue
;
18
}
19
20
// Static tool: full type inference
21
switch
(
toolCall
.
toolName
)
{
22
case
'weather'
:
23
console
.
log
(
toolCall
.
input
.
location
)
;
// typed as string
24
break
;
25
}
26
}
27
}
,
28
}
)
;
Preliminary Tool Results
You can return an
AsyncIterable
over multiple results.
In this case, the last value from the iterable is the final tool result.
This can be used in combination with generator functions to e.g. stream status information
during the tool execution:
1
tool
(
{
2
description
:
'Get the current weather.'
,
3
inputSchema
:
z
.
object
(
{
4
location
:
z
.
string
(
)
,
5
}
)
,
6
async
*
execute
(
{
location
}
)
{
7
yield
{
8
status
:
'loading'
as
const
,
9
text
:
`
Getting weather for
${
location
}
`
,
10
weather
:
undefined
,
11
}
;
12
13
await
new
Promise
(
resolve
=>
setTimeout
(
resolve
,
3000
)
)
;
14
15
const
temperature
=
72
+
Math
.
floor
(
Math
.
random
(
)
*
21
)
-
10
;
16
17
yield
{
18
status
:
'success'
as
const
,
19
text
:
`
The weather in
${
location
}
is
${
temperature
}
°F
`
,
20
temperature
,
21
}
;
22
}
,
23
}
)
;
Tool Choice
You can use the
toolChoice
setting to influence when a tool is selected.
It supports the following settings:
auto
(default): the model can choose whether and which tools to call.
required
: the model must call a tool. It can choose which tool to call.
none
: the model must not call tools
{ type: 'tool', toolName: string (typed) }
: the model must call the specified tool
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
z
}
from
'zod'
;
2
import
{
generateText
,
tool
}
from
'ai'
;
3
4
const
result
=
await
generateText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
tools
:
{
7
weather
:
tool
(
{
8
description
:
'Get the weather in a location'
,
9
inputSchema
:
z
.
object
(
{
10
location
:
z
.
string
(
)
.
describe
(
'The location to get the weather for'
)
,
11
}
)
,
12
execute
:
async
(
{
location
}
)
=>
(
{
13
location
,
14
temperature
:
72
+
Math
.
floor
(
Math
.
random
(
)
*
21
)
-
10
,
15
}
)
,
16
}
)
,
17
}
,
18
toolChoice
:
'required'
,
// force the model to call a tool
19
prompt
:
'What is the weather in San Francisco?'
,
20
}
)
;
Tool Execution Options
When tools are called, they receive additional options as a second parameter.
Tool Call ID
The ID of the tool call is forwarded to the tool execution.
You can use it e.g. when sending tool-call related information with stream data.
1
import
{
2
streamText
,
3
tool
,
4
createUIMessageStream
,
5
createUIMessageStreamResponse
,
6
}
from
'ai'
;
7
8
export
async
function
POST
(
req
:
Request
)
{
9
const
{
messages
}
=
await
req
.
json
(
)
;
10
11
const
stream
=
createUIMessageStream
(
{
12
execute
:
(
{
writer
}
)
=>
{
13
const
result
=
streamText
(
{
14
// ...
15
messages
,
16
tools
:
{
17
myTool
:
tool
(
{
18
// ...
19
execute
:
async
(
args
,
{
toolCallId
}
)
=>
{
20
// return e.g. custom status for tool call
21
writer
.
write
(
{
22
type
:
'data-tool-status'
,
23
id
:
toolCallId
,
24
data
:
{
25
name
:
'myTool'
,
26
status
:
'in-progress'
,
27
}
,
28
}
)
;
29
// ...
30
}
,
31
}
)
,
32
}
,
33
}
)
;
34
35
writer
.
merge
(
result
.
toUIMessageStream
(
)
)
;
36
}
,
37
}
)
;
38
39
return
createUIMessageStreamResponse
(
{
stream
}
)
;
40
}
Messages
The messages that were sent to the language model to initiate the response that contained the tool call are forwarded to the tool execution.
You can access them in the second parameter of the
execute
function.
In multi-step calls, the messages contain the text, tool calls, and tool results from all previous steps.
1
import
{
generateText
,
tool
}
from
'ai'
;
2
3
const
result
=
await
generateText
(
{
4
// ...
5
tools
:
{
6
myTool
:
tool
(
{
7
// ...
8
execute
:
async
(
args
,
{
messages
}
)
=>
{
9
// use the message history in e.g. calls to other language models
10
return
{
...
}
;
11
}
,
12
}
)
,
13
}
,
14
}
)
;
Abort Signals
The abort signals from
generateText
and
streamText
are forwarded to the tool execution.
You can access them in the second parameter of the
execute
function and e.g. abort long-running computations or forward them to fetch calls inside tools.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
z
}
from
'zod'
;
2
import
{
generateText
,
tool
}
from
'ai'
;
3
4
const
result
=
await
generateText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
abortSignal
:
myAbortSignal
,
// signal that will be forwarded to tools
7
tools
:
{
8
weather
:
tool
(
{
9
description
:
'Get the weather in a location'
,
10
inputSchema
:
z
.
object
(
{
location
:
z
.
string
(
)
}
)
,
11
execute
:
async
(
{
location
}
,
{
abortSignal
}
)
=>
{
12
return
fetch
(
13
`
https://api.weatherapi.com/v1/current.json?q=
${
location
}
`
,
14
{
signal
:
abortSignal
}
,
// forward the abort signal to fetch
15
)
;
16
}
,
17
}
)
,
18
}
,
19
prompt
:
'What is the weather in San Francisco?'
,
20
}
)
;
Context (experimental)
You can pass in arbitrary context from
generateText
or
streamText
via the
experimental_context
setting.
This context is available in the
experimental_context
tool execution option.
1
const
result
=
await
generateText
(
{
2
// ...
3
tools
:
{
4
someTool
:
tool
(
{
5
// ...
6
execute
:
async
(
input
,
{
experimental_context
:
context
}
)
=>
{
7
const
typedContext
=
context
as
{
example
:
string
}
;
// or use type validation library
8
// ...
9
}
,
10
}
)
,
11
}
,
12
experimental_context
:
{
example
:
'123'
}
,
13
}
)
;
Tool Input Lifecycle Hooks
The following tool input lifecycle hooks are available:
onInputStart
: Called when the model starts generating the input (arguments) for the tool call
onInputDelta
: Called for each chunk of text as the input is streamed
onInputAvailable
: Called when the complete input is available and validated
onInputStart
and
onInputDelta
are only called in streaming contexts (when using
streamText
). They are not called when using
generateText
.
Example
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
,
tool
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
result
=
streamText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
tools
:
{
7
getWeather
:
tool
(
{
8
description
:
'Get the weather in a location'
,
9
inputSchema
:
z
.
object
(
{
10
location
:
z
.
string
(
)
.
describe
(
'The location to get the weather for'
)
,
11
}
)
,
12
execute
:
async
(
{
location
}
)
=>
(
{
13
temperature
:
72
+
Math
.
floor
(
Math
.
random
(
)
*
21
)
-
10
,
14
}
)
,
15
onInputStart
:
(
)
=>
{
16
console
.
log
(
'Tool call starting'
)
;
17
}
,
18
onInputDelta
:
(
{
inputTextDelta
}
)
=>
{
19
console
.
log
(
'Received input chunk:'
,
inputTextDelta
)
;
20
}
,
21
onInputAvailable
:
(
{
input
}
)
=>
{
22
console
.
log
(
'Complete input:'
,
input
)
;
23
}
,
24
}
)
,
25
}
,
26
prompt
:
'What is the weather in San Francisco?'
,
27
}
)
;
Types
Modularizing your code often requires defining types to ensure type safety and reusability.
To enable this, the AI SDK provides several helper types for tools, tool calls, and tool results.
You can use them to strongly type your variables, function parameters, and return types
in parts of the code that are not directly related to
streamText
or
generateText
.
Each tool call is typed with
ToolCall<NAME extends string, ARGS>
, depending
on the tool that has been invoked.
Similarly, the tool results are typed with
ToolResult<NAME extends string, ARGS, RESULT>
.
The tools in
streamText
and
generateText
are defined as a
ToolSet
.
The type inference helpers
TypedToolCall<TOOLS extends ToolSet>
and
TypedToolResult<TOOLS extends ToolSet>
can be used to
extract the tool call and tool result types from the tools.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
TypedToolCall
,
TypedToolResult
,
generateText
,
tool
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
myToolSet
=
{
5
firstTool
:
tool
(
{
6
description
:
'Greets the user'
,
7
inputSchema
:
z
.
object
(
{
name
:
z
.
string
(
)
}
)
,
8
execute
:
async
(
{
name
}
)
=>
`
Hello,
${
name
}
!
`
,
9
}
)
,
10
secondTool
:
tool
(
{
11
description
:
'Tells the user their age'
,
12
inputSchema
:
z
.
object
(
{
age
:
z
.
number
(
)
}
)
,
13
execute
:
async
(
{
age
}
)
=>
`
You are
${
age
}
years old!
`
,
14
}
)
,
15
}
;
16
17
type
MyToolCall
=
TypedToolCall
<
typeof
myToolSet
>
;
18
type
MyToolResult
=
TypedToolResult
<
typeof
myToolSet
>
;
19
20
async
function
generateSomething
(
prompt
:
string
)
:
Promise
<
{
21
text
:
string
;
22
toolCalls
:
Array
<
MyToolCall
>
;
// typed tool calls
23
toolResults
:
Array
<
MyToolResult
>
;
// typed tool results
24
}
>
{
25
return
generateText
(
{
26
model
:
"anthropic/claude-sonnet-4.5"
,
27
tools
:
myToolSet
,
28
prompt
,
29
}
)
;
30
}
Handling Errors
The AI SDK has three tool-call related errors:
NoSuchToolError
: the model tries to call a tool that is not defined in the tools object
InvalidToolInputError
: the model calls a tool with inputs that do not match the tool's input schema
ToolCallRepairError
: an error that occurred during tool call repair
When tool execution fails (errors thrown by your tool's
execute
function), the AI SDK adds them as
tool-error
content parts to enable automated LLM roundtrips in multi-step scenarios.
generateText
generateText
throws errors for tool schema validation issues and other errors, and can be handled using a
try
/
catch
block. Tool execution errors appear as
tool-error
parts in the result steps:
1
try
{
2
const
result
=
await
generateText
(
{
3
//...
4
}
)
;
5
}
catch
(
error
)
{
6
if
(
NoSuchToolError
.
isInstance
(
error
)
)
{
7
// handle the no such tool error
8
}
else
if
(
InvalidToolInputError
.
isInstance
(
error
)
)
{
9
// handle the invalid tool inputs error
10
}
else
{
11
// handle other errors
12
}
13
}
Tool execution errors are available in the result steps:
1
const
{
steps
}
=
await
generateText
(
{
2
// ...
3
}
)
;
4
5
// check for tool errors in the steps
6
const
toolErrors
=
steps
.
flatMap
(
step
=>
7
step
.
content
.
filter
(
part
=>
part
.
type
===
'tool-error'
)
,
8
)
;
9
10
toolErrors
.
forEach
(
toolError
=>
{
11
console
.
log
(
'Tool error:'
,
toolError
.
error
)
;
12
console
.
log
(
'Tool name:'
,
toolError
.
toolName
)
;
13
console
.
log
(
'Tool input:'
,
toolError
.
input
)
;
14
}
)
;
streamText
streamText
sends errors as part of the full stream. Tool execution errors appear as
tool-error
parts, while other errors appear as
error
parts.
When using
toUIMessageStreamResponse
, you can pass an
onError
function to extract the error message from the error part and forward it as part of the stream response:
1
const
result
=
streamText
(
{
2
// ...
3
}
)
;
4
5
return
result
.
toUIMessageStreamResponse
(
{
6
onError
:
error
=>
{
7
if
(
NoSuchToolError
.
isInstance
(
error
)
)
{
8
return
'The model tried to call a unknown tool.'
;
9
}
else
if
(
InvalidToolInputError
.
isInstance
(
error
)
)
{
10
return
'The model called a tool with invalid inputs.'
;
11
}
else
{
12
return
'An unknown error occurred.'
;
13
}
14
}
,
15
}
)
;
Tool Call Repair
The tool call repair feature is experimental and may change in the future.
Language models sometimes fail to generate valid tool calls,
especially when the input schema is complex or the model is smaller.
If you use multiple steps, those failed tool calls will be sent back to the LLM
in the next step to give it an opportunity to fix it.
However, you may want to control how invalid tool calls are repaired without requiring
additional steps that pollute the message history.
You can use the
experimental_repairToolCall
function to attempt to repair the tool call
with a custom function.
You can use different strategies to repair the tool call:
Use a model with structured outputs to generate the inputs.
Send the messages, system prompt, and tool schema to a stronger model to generate the inputs.
Provide more specific repair instructions based on which tool was called.
Example: Use a model with structured outputs for repair
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
generateObject
,
generateText
,
NoSuchToolError
,
tool
}
from
'ai'
;
3
4
const
result
=
await
generateText
(
{
5
model
,
6
tools
,
7
prompt
,
8
9
experimental_repairToolCall
:
async
(
{
10
toolCall
,
11
tools
,
12
inputSchema
,
13
error
,
14
}
)
=>
{
15
if
(
NoSuchToolError
.
isInstance
(
error
)
)
{
16
return
null
;
// do not attempt to fix invalid tool names
17
}
18
19
const
tool
=
tools
[
toolCall
.
toolName
as
keyof
typeof
tools
]
;
20
21
const
{
object
:
repairedArgs
}
=
await
generateObject
(
{
22
model
:
"anthropic/claude-sonnet-4.5"
,
23
schema
:
tool
.
inputSchema
,
24
prompt
:
[
25
`
The model tried to call the tool "
${
toolCall
.
toolName
}
"
`
+
26
`
with the following inputs:
`
,
27
JSON
.
stringify
(
toolCall
.
input
)
,
28
`
The tool accepts the following schema:
`
,
29
JSON
.
stringify
(
inputSchema
(
toolCall
)
)
,
30
'Please fix the inputs.'
,
31
]
.
join
(
'\n'
)
,
32
}
)
;
33
34
return
{
...
toolCall
,
input
:
JSON
.
stringify
(
repairedArgs
)
}
;
35
}
,
36
}
)
;
Example: Use the re-ask strategy for repair
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
generateObject
,
generateText
,
NoSuchToolError
,
tool
}
from
'ai'
;
3
4
const
result
=
await
generateText
(
{
5
model
,
6
tools
,
7
prompt
,
8
9
experimental_repairToolCall
:
async
(
{
10
toolCall
,
11
tools
,
12
error
,
13
messages
,
14
system
,
15
}
)
=>
{
16
const
result
=
await
generateText
(
{
17
model
,
18
system
,
19
messages
:
[
20
...
messages
,
21
{
22
role
:
'assistant'
,
23
content
:
[
24
{
25
type
:
'tool-call'
,
26
toolCallId
:
toolCall
.
toolCallId
,
27
toolName
:
toolCall
.
toolName
,
28
input
:
toolCall
.
input
,
29
}
,
30
]
,
31
}
,
32
{
33
role
:
'tool'
as
const
,
34
content
:
[
35
{
36
type
:
'tool-result'
,
37
toolCallId
:
toolCall
.
toolCallId
,
38
toolName
:
toolCall
.
toolName
,
39
output
:
error
.
message
,
40
}
,
41
]
,
42
}
,
43
]
,
44
tools
,
45
}
)
;
46
47
const
newToolCall
=
result
.
toolCalls
.
find
(
48
newToolCall
=>
newToolCall
.
toolName
===
toolCall
.
toolName
,
49
)
;
50
51
return
newToolCall
!=
null
52
?
{
53
toolCallType
:
'function'
as
const
,
54
toolCallId
:
toolCall
.
toolCallId
,
55
toolName
:
toolCall
.
toolName
,
56
input
:
JSON
.
stringify
(
newToolCall
.
input
)
,
57
}
58
:
null
;
59
}
,
60
}
)
;
Active Tools
Language models can only handle a limited number of tools at a time, depending on the model.
To allow for static typing using a large number of tools and limiting the available tools to the model at the same time,
the AI SDK provides the
activeTools
property.
It is an array of tool names that are currently active.
By default, the value is
undefined
and all tools are active.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
generateText
}
from
'ai'
;
3
4
const
{
text
}
=
await
generateText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
tools
:
myToolSet
,
7
activeTools
:
[
'firstTool'
]
,
8
}
)
;
Multi-modal Tool Results
Multi-modal tool results are experimental and only supported by Anthropic and
OpenAI.
In order to send multi-modal tool results, e.g. screenshots, back to the model,
they need to be converted into a specific format.
AI SDK Core tools have an optional
toModelOutput
function
that converts the tool result into a content part.
Here is an example for converting a screenshot into a content part:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateText
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
tools
:
{
4
computer
:
anthropic
.
tools
.
computer_20241022
(
{
5
// ...
6
async
execute
(
{
action
,
coordinate
,
text
}
)
{
7
switch
(
action
)
{
8
case
'screenshot'
:
{
9
return
{
10
type
:
'image'
,
11
data
:
fs
12
.
readFileSync
(
'./data/screenshot-editor.png'
)
13
.
toString
(
'base64'
)
,
14
}
;
15
}
16
default
:
{
17
return
`
executed
${
action
}
`
;
18
}
19
}
20
}
,
21
22
// map to tool result content for LLM consumption:
23
toModelOutput
(
{
output
}
)
{
24
return
{
25
type
:
'content'
,
26
value
:
27
typeof
output
===
'string'
28
?
[
{
type
:
'text'
,
text
:
output
}
]
29
:
[
{
type
:
'media'
,
data
:
output
.
data
,
mediaType
:
'image/png'
}
]
,
30
}
;
31
}
,
32
}
)
,
33
}
,
34
// ...
35
}
)
;
Extracting Tools
Once you start having many tools, you might want to extract them into separate files.
The
tool
helper function is crucial for this, because it ensures correct type inference.
Here is an example of an extracted tool:
tools/weather-tool.ts
1
import
{
tool
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
// the `tool` helper function ensures correct type inference:
5
export
const
weatherTool
=
tool
(
{
6
description
:
'Get the weather in a location'
,
7
inputSchema
:
z
.
object
(
{
8
location
:
z
.
string
(
)
.
describe
(
'The location to get the weather for'
)
,
9
}
)
,
10
execute
:
async
(
{
location
}
)
=>
(
{
11
location
,
12
temperature
:
72
+
Math
.
floor
(
Math
.
random
(
)
*
21
)
-
10
,
13
}
)
,
14
}
)
;
MCP Tools
The AI SDK supports connecting to Model Context Protocol (MCP) servers to access their tools.
MCP enables your AI applications to discover and use tools across various services through a standardized interface.
For detailed information about MCP tools, including initialization, transport options, and usage patterns, see the
MCP Tools documentation
.
AI SDK Tools vs MCP Tools
In most cases, you should define your own AI SDK tools for production applications. They provide full control, type safety, and optimal performance. MCP tools are best suited for rapid development iteration and scenarios where users bring their own tools.
Aspect
AI SDK Tools
MCP Tools
Type Safety
Full static typing end-to-end
Dynamic discovery at runtime
Execution
Same process as your request (low latency)
Separate server (network overhead)
Prompt Control
Full control over descriptions and schemas
Controlled by MCP server owner
Schema Control
You define and optimize for your model
Controlled by MCP server owner
Version Management
Full visibility over updates
Can update independently (version skew risk)
Authentication
Same process, no additional auth required
Separate server introduces additional auth complexity
Best For
Production applications requiring control and performance
Development iteration, user-provided tools
Examples
You can see tools in action using various frameworks in the following examples:
Learn to use tools in Node.js
Learn to use tools in Next.js with Route Handlers
Learn to use MCP tools in Node.js
Previous
Generating Structured Data
Next
Model Context Protocol (MCP)
On this page
Tool Calling
Strict Mode
Input Examples
Tool Execution Approval
How It Works
Handling Approval Requests
Dynamic Approval
Tool Execution Approval with useChat
Multi-Step Calls (using stopWhen)
Example
Steps
Example: Extract tool results from all steps
onStepFinish callback
prepareStep callback
Message Modification for Longer Agentic Loops
Provider Options for Step Configuration
Response Messages
Dynamic Tools
Using dynamicTool
Type-Safe Handling
Preliminary Tool Results
Tool Choice
Tool Execution Options
Tool Call ID
Messages
Abort Signals
Context (experimental)
Tool Input Lifecycle Hooks
Example
Types
Handling Errors
generateText
streamText
Tool Call Repair
Example: Use a model with structured outputs for repair
Example: Use the re-ask strategy for repair
Active Tools
Multi-modal Tool Results
Extracting Tools
MCP Tools
AI SDK Tools vs MCP Tools
Examples
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== MCP-TOOLS =====

AI SDK Core: Model Context Protocol (MCP)
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Model Context Protocol (MCP)
Copy markdown
Model Context Protocol (MCP)
The AI SDK supports connecting to
Model Context Protocol (MCP)
servers to access their tools, resources, and prompts.
This enables your AI applications to discover and use capabilities across various services through a standardized interface.
If you're using OpenAI's Responses API, you can also use the built-in
openai.tools.mcp
tool, which provides direct MCP server integration without
needing to convert tools. See the
OpenAI provider
documentation
for details.
Initializing an MCP Client
We recommend using HTTP transport (like
StreamableHTTPClientTransport
) for production deployments. The stdio transport should only be used for connecting to local servers as it cannot be deployed to production environments.
Create an MCP client using one of the following transport options:
HTTP transport (Recommended)
: Either configure HTTP directly via the client using
transport: { type: 'http', ... }
, or use MCP's official TypeScript SDK
StreamableHTTPClientTransport
SSE (Server-Sent Events): An alternative HTTP-based transport
stdio
: For local development only. Uses standard input/output streams for local MCP servers
HTTP Transport (Recommended)
For production deployments, we recommend using the HTTP transport. You can configure it directly on the client:
1
import
{
createMCPClient
}
from
'@ai-sdk/mcp'
;
2
3
const
mcpClient
=
await
createMCPClient
(
{
4
transport
:
{
5
type
:
'http'
,
6
url
:
'https://your-server.com/mcp'
,
7
8
// optional: configure HTTP headers
9
headers
:
{
Authorization
:
'Bearer my-api-key'
}
,
10
11
// optional: provide an OAuth client provider for automatic authorization
12
authProvider
:
myOAuthClientProvider
,
13
}
,
14
}
)
;
Alternatively, you can use
StreamableHTTPClientTransport
from MCP's official TypeScript SDK:
1
import
{
createMCPClient
}
from
'@ai-sdk/mcp'
;
2
import
{
StreamableHTTPClientTransport
}
from
'@modelcontextprotocol/sdk/client/streamableHttp.js'
;
3
4
const
url
=
new
URL
(
'https://your-server.com/mcp'
)
;
5
const
mcpClient
=
await
createMCPClient
(
{
6
transport
:
new
StreamableHTTPClientTransport
(
url
,
{
7
sessionId
:
'session_123'
,
8
}
)
,
9
}
)
;
SSE Transport
SSE provides an alternative HTTP-based transport option. Configure it with a
type
and
url
property. You can also provide an
authProvider
for OAuth:
1
import
{
createMCPClient
}
from
'@ai-sdk/mcp'
;
2
3
const
mcpClient
=
await
createMCPClient
(
{
4
transport
:
{
5
type
:
'sse'
,
6
url
:
'https://my-server.com/sse'
,
7
8
// optional: configure HTTP headers
9
headers
:
{
Authorization
:
'Bearer my-api-key'
}
,
10
11
// optional: provide an OAuth client provider for automatic authorization
12
authProvider
:
myOAuthClientProvider
,
13
}
,
14
}
)
;
Stdio Transport (Local Servers)
The stdio transport should only be used for local servers.
The Stdio transport can be imported from either the MCP SDK or the AI SDK:
1
import
{
createMCPClient
}
from
'@ai-sdk/mcp'
;
2
import
{
StdioClientTransport
}
from
'@modelcontextprotocol/sdk/client/stdio.js'
;
3
// Or use the AI SDK's stdio transport:
4
// import { Experimental_StdioMCPTransport as StdioClientTransport } from '@ai-sdk/mcp/mcp-stdio';
5
6
const
mcpClient
=
await
createMCPClient
(
{
7
transport
:
new
StdioClientTransport
(
{
8
command
:
'node'
,
9
args
:
[
'src/stdio/dist/server.js'
]
,
10
}
)
,
11
}
)
;
Custom Transport
You can also bring your own transport by implementing the
MCPTransport
interface for specific requirements not covered by the standard transports.
The client returned by the
createMCPClient
function is a
lightweight client intended for use in tool conversion. It currently does not
support all features of the full MCP client, such as: session
management, resumable streams, and receiving notifications.
Authorization via OAuth is supported when using the AI SDK MCP HTTP or SSE
transports by providing an
authProvider
.
Closing the MCP Client
After initialization, you should close the MCP client based on your usage pattern:
For short-lived usage (e.g., single requests), close the client when the response is finished
For long-running clients (e.g., command line apps), keep the client open but ensure it's closed when the application terminates
When streaming responses, you can close the client when the LLM response has finished. For example, when using
streamText
, you should use the
onFinish
callback:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
mcpClient
=
await
createMCPClient
(
{
2
// ...
3
}
)
;
4
5
const
tools
=
await
mcpClient
.
tools
(
)
;
6
7
const
result
=
await
streamText
(
{
8
model
:
"anthropic/claude-sonnet-4.5"
,
9
tools
,
10
prompt
:
'What is the weather in Brooklyn, New York?'
,
11
onFinish
:
async
(
)
=>
{
12
await
mcpClient
.
close
(
)
;
13
}
,
14
}
)
;
When generating responses without streaming, you can use try/finally or cleanup functions in your framework:
1
let
mcpClient
:
MCPClient
|
undefined
;
2
3
try
{
4
mcpClient
=
await
createMCPClient
(
{
5
// ...
6
}
)
;
7
}
finally
{
8
await
mcpClient
?.
close
(
)
;
9
}
Using MCP Tools
The client's
tools
method acts as an adapter between MCP tools and AI SDK tools. It supports two approaches for working with tool schemas:
Schema Discovery
With schema discovery, all tools offered by the server are automatically listed, and input parameter types are inferred based on the schemas provided by the server:
1
const
tools
=
await
mcpClient
.
tools
(
)
;
This approach is simpler to implement and automatically stays in sync with server changes. However, you won't have TypeScript type safety during development, and all tools from the server will be loaded
Schema Definition
For better type safety and control, you can define the tools and their input schemas explicitly in your client code:
1
import
{
z
}
from
'zod'
;
2
3
const
tools
=
await
mcpClient
.
tools
(
{
4
schemas
:
{
5
'get-data'
:
{
6
inputSchema
:
z
.
object
(
{
7
query
:
z
.
string
(
)
.
describe
(
'The data query'
)
,
8
format
:
z
.
enum
(
[
'json'
,
'text'
]
)
.
optional
(
)
,
9
}
)
,
10
}
,
11
// For tools with zero inputs, you should use an empty object:
12
'tool-with-no-args'
:
{
13
inputSchema
:
z
.
object
(
{
}
)
,
14
}
,
15
}
,
16
}
)
;
This approach provides full TypeScript type safety and IDE autocompletion, letting you catch parameter mismatches during development. When you define
schemas
, the client only pulls the explicitly defined tools, keeping your application focused on the tools it needs
Typed Tool Outputs
When MCP servers return
structuredContent
(per the
MCP specification
), you can define an
outputSchema
to get typed tool results:
1
import
{
z
}
from
'zod'
;
2
3
const
tools
=
await
mcpClient
.
tools
(
{
4
schemas
:
{
5
'get-weather'
:
{
6
inputSchema
:
z
.
object
(
{
7
location
:
z
.
string
(
)
,
8
}
)
,
9
// Define outputSchema for typed results
10
outputSchema
:
z
.
object
(
{
11
temperature
:
z
.
number
(
)
,
12
conditions
:
z
.
string
(
)
,
13
humidity
:
z
.
number
(
)
,
14
}
)
,
15
}
,
16
}
,
17
}
)
;
18
19
const
result
=
await
tools
[
'get-weather'
]
.
execute
(
20
{
location
:
'New York'
}
,
21
{
messages
:
[
]
,
toolCallId
:
'weather-1'
}
,
22
)
;
23
24
console
.
log
(
`
Temperature:
${
result
.
temperature
}
°C
`
)
;
When
outputSchema
is provided:
The client extracts
structuredContent
from the tool result
The output is validated against your schema at runtime
You get full TypeScript type safety for the result
If the server doesn't return
structuredContent
, the client falls back to parsing JSON from the text content. If neither is available or validation fails, an error is thrown.
Without
outputSchema
, the tool returns the raw
CallToolResult
object
containing
content
and optional
isError
fields.
Using MCP Resources
According to the
MCP specification
, resources are
application-driven
data sources that provide context to the model. Unlike tools (which are model-controlled), your application decides when to fetch and pass resources as context.
The MCP client provides three methods for working with resources:
Listing Resources
List all available resources from the MCP server:
1
const
resources
=
await
mcpClient
.
listResources
(
)
;
Reading Resource Contents
Read the contents of a specific resource by its URI:
1
const
resourceData
=
await
mcpClient
.
readResource
(
{
2
uri
:
'file:///example/document.txt'
,
3
}
)
;
Listing Resource Templates
Resource templates are dynamic URI patterns that allow flexible queries. List all available templates:
1
const
templates
=
await
mcpClient
.
listResourceTemplates
(
)
;
Using MCP Prompts
MCP Prompts is an experimental feature and may change in the future.
According to the MCP specification, prompts are user-controlled templates that servers expose for clients to list and retrieve with optional arguments.
Listing Prompts
1
const
prompts
=
await
mcpClient
.
experimental_listPrompts
(
)
;
Getting a Prompt
Retrieve prompt messages, optionally passing arguments defined by the server:
1
const
prompt
=
await
mcpClient
.
experimental_getPrompt
(
{
2
name
:
'code_review'
,
3
arguments
:
{
code
:
'function add(a, b) { return a + b; }'
}
,
4
}
)
;
Handling Elicitation Requests
Elicitation is a mechanism where MCP servers can request additional information from the client during tool execution. For example, a server might need user input to complete a registration form or confirmation for a sensitive operation.
It is up to the client application to handle elicitation requests properly.
The MCP client simply surfaces these requests from the server to your
application code.
Enabling Elicitation Support
To enable elicitation, you need to advertise the capability when creating the MCP client:
1
const
mcpClient
=
await
createMCPClient
(
{
2
transport
:
{
3
type
:
'sse'
,
4
url
:
'https://your-server.com/sse'
,
5
}
,
6
capabilities
:
{
7
elicitation
:
{
}
,
8
}
,
9
}
)
;
Registering an Elicitation Handler
Use the
onElicitationRequest
method to register a handler that will be called when the server requests input:
1
import
{
ElicitationRequestSchema
}
from
'@ai-sdk/mcp'
;
2
3
mcpClient
.
onElicitationRequest
(
ElicitationRequestSchema
,
async
request
=>
{
4
// request.params.message: A message describing what input is needed
5
// request.params.requestedSchema: JSON schema defining the expected input structure
6
7
// Get input from the user (implement according to your application's needs)
8
const
userInput
=
await
getInputFromUser
(
9
request
.
params
.
message
,
10
request
.
params
.
requestedSchema
,
11
)
;
12
13
// Return the result with one of three actions:
14
return
{
15
action
:
'accept'
,
// or 'decline' or 'cancel'
16
content
:
userInput
,
// only required when action is 'accept'
17
}
;
18
}
)
;
Elicitation Response Actions
Your handler must return an object with an
action
field that can be one of:
'accept'
: User provided the requested information. Must include
content
with the data.
'decline'
: User chose not to provide the information.
'cancel'
: User cancelled the operation entirely.
Examples
You can see MCP in action in the following examples:
Learn to use MCP tools in Node.js
Learn to handle MCP elicitation requests in Node.js
Previous
Tool Calling
Next
Prompt Engineering
On this page
Model Context Protocol (MCP)
Initializing an MCP Client
HTTP Transport (Recommended)
SSE Transport
Stdio Transport (Local Servers)
Custom Transport
Closing the MCP Client
Using MCP Tools
Schema Discovery
Schema Definition
Typed Tool Outputs
Using MCP Resources
Listing Resources
Reading Resource Contents
Listing Resource Templates
Using MCP Prompts
Listing Prompts
Getting a Prompt
Handling Elicitation Requests
Enabling Elicitation Support
Registering an Elicitation Handler
Elicitation Response Actions
Examples
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== PROMPT-ENGINEERING =====

AI SDK Core: Prompt Engineering
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Prompt Engineering
Copy markdown
Prompt Engineering
Tips
Prompts for Tools
When you create prompts that include tools, getting good results can be tricky as the number and complexity of your tools increases.
Here are a few tips to help you get the best results:
Use a model that is strong at tool calling, such as
gpt-5
or
gpt-4.1
. Weaker models will often struggle to call tools effectively and flawlessly.
Keep the number of tools low, e.g. to 5 or less.
Keep the complexity of the tool parameters low. Complex Zod schemas with many nested and optional elements, unions, etc. can be challenging for the model to work with.
Use semantically meaningful names for your tools, parameters, parameter properties, etc. The more information you pass to the model, the better it can understand what you want.
Add
.describe("...")
to your Zod schema properties to give the model hints about what a particular property is for.
When the output of a tool might be unclear to the model and there are dependencies between tools, use the
description
field of a tool to provide information about the output of the tool execution.
You can include example input/outputs of tool calls in your prompt to help the model understand how to use the tools. Keep in mind that the tools work with JSON objects, so the examples should use JSON.
In general, the goal should be to give the model all information it needs in a clear way.
Tool & Structured Data Schemas
The mapping from Zod schemas to LLM inputs (typically JSON schema) is not always straightforward, since the mapping is not one-to-one.
Zod Dates
Zod expects JavaScript Date objects, but models return dates as strings.
You can specify and validate the date format using
z.string().datetime()
or
z.string().date()
,
and then use a Zod transformer to convert the string to a Date object.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateObject
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
schema
:
z
.
object
(
{
4
events
:
z
.
array
(
5
z
.
object
(
{
6
event
:
z
.
string
(
)
,
7
date
:
z
8
.
string
(
)
9
.
date
(
)
10
.
transform
(
value
=>
new
Date
(
value
)
)
,
11
}
)
,
12
)
,
13
}
)
,
14
prompt
:
'List 5 important events from the year 2000.'
,
15
}
)
;
Optional Parameters
When working with tools that have optional parameters, you may encounter compatibility issues with certain providers that use strict schema validation.
This is particularly relevant for OpenAI models with structured outputs
(strict mode).
For maximum compatibility, optional parameters should use
.nullable()
instead of
.optional()
:
1
// This may fail with strict schema validation
2
const
failingTool
=
tool
(
{
3
description
:
'Execute a command'
,
4
inputSchema
:
z
.
object
(
{
5
command
:
z
.
string
(
)
,
6
workdir
:
z
.
string
(
)
.
optional
(
)
,
// This can cause errors
7
timeout
:
z
.
string
(
)
.
optional
(
)
,
8
}
)
,
9
}
)
;
10
11
// This works with strict schema validation
12
const
workingTool
=
tool
(
{
13
description
:
'Execute a command'
,
14
inputSchema
:
z
.
object
(
{
15
command
:
z
.
string
(
)
,
16
workdir
:
z
.
string
(
)
.
nullable
(
)
,
// Use nullable instead
17
timeout
:
z
.
string
(
)
.
nullable
(
)
,
18
}
)
,
19
}
)
;
Temperature Settings
For tool calls and object generation, it's recommended to use
temperature: 0
to ensure deterministic and consistent results:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateText
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
temperature
:
0
,
// Recommended for tool calls
4
tools
:
{
5
myTool
:
tool
(
{
6
description
:
'Execute a command'
,
7
inputSchema
:
z
.
object
(
{
8
command
:
z
.
string
(
)
,
9
}
)
,
10
}
)
,
11
}
,
12
prompt
:
'Execute the ls command'
,
13
}
)
;
Lower temperature values reduce randomness in model outputs, which is particularly important when the model needs to:
Generate structured data with specific formats
Make precise tool calls with correct parameters
Follow strict schemas consistently
Debugging
Inspecting Warnings
Not all providers support all AI SDK features.
Providers either throw exceptions or return warnings when they do not support a feature.
To check if your prompt, tools, and settings are handled correctly by the provider, you can check the call warnings:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateText
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
prompt
:
'Hello, world!'
,
4
}
)
;
5
6
console
.
log
(
result
.
warnings
)
;
HTTP Request Bodies
You can inspect the raw HTTP request bodies for models that expose them, e.g.
OpenAI
.
This allows you to inspect the exact payload that is sent to the model provider in the provider-specific way.
Request bodies are available via the
request.body
property of the response:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateText
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
prompt
:
'Hello, world!'
,
4
}
)
;
5
6
console
.
log
(
result
.
request
.
body
)
;
Previous
Model Context Protocol (MCP)
Next
Settings
On this page
Prompt Engineering
Tips
Prompts for Tools
Tool & Structured Data Schemas
Zod Dates
Optional Parameters
Temperature Settings
Debugging
Inspecting Warnings
HTTP Request Bodies
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== SETTINGS =====

AI SDK Core: Settings
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Settings
Copy markdown
Settings
Large language models (LLMs) typically provide settings to augment their output.
All AI SDK functions support the following common settings in addition to the model, the
prompt
, and additional provider-specific settings:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateText
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
maxOutputTokens
:
512
,
4
temperature
:
0.3
,
5
maxRetries
:
5
,
6
prompt
:
'Invent a new holiday and describe its traditions.'
,
7
}
)
;
Some providers do not support all common settings. If you use a setting with a
provider that does not support it, a warning will be generated. You can check
the
warnings
property in the result object to see if any warnings were
generated.
maxOutputTokens
Maximum number of tokens to generate.
temperature
Temperature setting.
The value is passed through to the provider. The range depends on the provider and model.
For most providers,
0
means almost deterministic results, and higher values mean more randomness.
It is recommended to set either
temperature
or
topP
, but not both.
In AI SDK 5.0, temperature is no longer set to
0
by default.
topP
Nucleus sampling.
The value is passed through to the provider. The range depends on the provider and model.
For most providers, nucleus sampling is a number between 0 and 1.
E.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.
It is recommended to set either
temperature
or
topP
, but not both.
topK
Only sample from the top K options for each subsequent token.
Used to remove "long tail" low probability responses.
Recommended for advanced use cases only. You usually only need to use
temperature
.
presencePenalty
The presence penalty affects the likelihood of the model to repeat information that is already in the prompt.
The value is passed through to the provider. The range depends on the provider and model.
For most providers,
0
means no penalty.
frequencyPenalty
The frequency penalty affects the likelihood of the model to repeatedly use the same words or phrases.
The value is passed through to the provider. The range depends on the provider and model.
For most providers,
0
means no penalty.
stopSequences
The stop sequences to use for stopping the text generation.
If set, the model will stop generating text when one of the stop sequences is generated.
Providers may have limits on the number of stop sequences.
seed
It is the seed (integer) to use for random sampling.
If set and supported by the model, calls will generate deterministic results.
maxRetries
Maximum number of retries. Set to 0 to disable retries. Default:
2
.
abortSignal
An optional abort signal that can be used to cancel the call.
The abort signal can e.g. be forwarded from a user interface to cancel the call,
or to define a timeout using
AbortSignal.timeout
.
Example: AbortSignal.timeout
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateText
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
prompt
:
'Invent a new holiday and describe its traditions.'
,
4
abortSignal
:
AbortSignal
.
timeout
(
5000
)
,
// 5 seconds
5
}
)
;
timeout
An optional timeout in milliseconds. The call will be aborted if it takes longer than the specified duration.
This is a convenience parameter that creates an abort signal internally. It can be used alongside
abortSignal
- if both are provided, the call will abort when either condition is met.
You can specify the timeout either as a number (milliseconds) or as an object with a
totalMs
property.
Example: 5 second timeout (number format)
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateText
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
prompt
:
'Invent a new holiday and describe its traditions.'
,
4
timeout
:
5000
,
// 5 seconds
5
}
)
;
Example: 5 second timeout (object format)
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateText
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
prompt
:
'Invent a new holiday and describe its traditions.'
,
4
timeout
:
{
totalMs
:
5000
}
,
// 5 seconds
5
}
)
;
headers
Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.
You can use the request headers to provide additional information to the provider,
depending on what the provider supports. For example, some observability providers support
headers such as
Prompt-Id
.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
}
from
'ai'
;
2
3
const
result
=
await
generateText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
prompt
:
'Invent a new holiday and describe its traditions.'
,
6
headers
:
{
7
'Prompt-Id'
:
'my-prompt-id'
,
8
}
,
9
}
)
;
The
headers
setting is for request-specific headers. You can also set
headers
in the provider configuration. These headers will be sent with every
request made by the provider.
Previous
Prompt Engineering
Next
Embeddings
On this page
Settings
maxOutputTokens
temperature
topP
topK
presencePenalty
frequencyPenalty
stopSequences
seed
maxRetries
abortSignal
Example: AbortSignal.timeout
timeout
Example: 5 second timeout (number format)
Example: 5 second timeout (object format)
headers
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== EMBEDDINGS =====

AI SDK Core: Embeddings
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Embeddings
Copy markdown
Embeddings
Embeddings are a way to represent words, phrases, or images as vectors in a high-dimensional space.
In this space, similar words are close to each other, and the distance between words can be used to measure their similarity.
Embedding a Single Value
The AI SDK provides the
embed
function to embed single values, which is useful for tasks such as finding similar words
or phrases or clustering text.
You can use it with embeddings models, e.g.
openai.embeddingModel('text-embedding-3-large')
or
mistral.embeddingModel('mistral-embed')
.
1
import
{
embed
}
from
'ai'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
4
// 'embedding' is a single embedding object (number[])
5
const
{
embedding
}
=
await
embed
(
{
6
model
:
'openai/text-embedding-3-small'
,
7
value
:
'sunny day at the beach'
,
8
}
)
;
Embedding Many Values
When loading data, e.g. when preparing a data store for retrieval-augmented generation (RAG),
it is often useful to embed many values at once (batch embedding).
The AI SDK provides the
embedMany
function for this purpose.
Similar to
embed
, you can use it with embeddings models,
e.g.
openai.embeddingModel('text-embedding-3-large')
or
mistral.embeddingModel('mistral-embed')
.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
embedMany
}
from
'ai'
;
3
4
// 'embeddings' is an array of embedding objects (number[][]).
5
// It is sorted in the same order as the input values.
6
const
{
embeddings
}
=
await
embedMany
(
{
7
model
:
'openai/text-embedding-3-small'
,
8
values
:
[
9
'sunny day at the beach'
,
10
'rainy afternoon in the city'
,
11
'snowy night in the mountains'
,
12
]
,
13
}
)
;
Embedding Similarity
After embedding values, you can calculate the similarity between them using the
cosineSimilarity
function.
This is useful to e.g. find similar words or phrases in a dataset.
You can also rank and filter related items based on their similarity.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
cosineSimilarity
,
embedMany
}
from
'ai'
;
3
4
const
{
embeddings
}
=
await
embedMany
(
{
5
model
:
'openai/text-embedding-3-small'
,
6
values
:
[
'sunny day at the beach'
,
'rainy afternoon in the city'
]
,
7
}
)
;
8
9
console
.
log
(
10
`
cosine similarity:
${
cosineSimilarity
(
embeddings
[
0
]
,
embeddings
[
1
]
)
}
`
,
11
)
;
Token Usage
Many providers charge based on the number of tokens used to generate embeddings.
Both
embed
and
embedMany
provide token usage information in the
usage
property of the result object:
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
embed
}
from
'ai'
;
3
4
const
{
embedding
,
usage
}
=
await
embed
(
{
5
model
:
'openai/text-embedding-3-small'
,
6
value
:
'sunny day at the beach'
,
7
}
)
;
8
9
console
.
log
(
usage
)
;
// { tokens: 10 }
Settings
Provider Options
Embedding model settings can be configured using
providerOptions
for provider-specific parameters:
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
embed
}
from
'ai'
;
3
4
const
{
embedding
}
=
await
embed
(
{
5
model
:
'openai/text-embedding-3-small'
,
6
value
:
'sunny day at the beach'
,
7
providerOptions
:
{
8
openai
:
{
9
dimensions
:
512
,
// Reduce embedding dimensions
10
}
,
11
}
,
12
}
)
;
Parallel Requests
The
embedMany
function now supports parallel processing with configurable
maxParallelCalls
to optimize performance:
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
embedMany
}
from
'ai'
;
3
4
const
{
embeddings
,
usage
}
=
await
embedMany
(
{
5
maxParallelCalls
:
2
,
// Limit parallel requests
6
model
:
'openai/text-embedding-3-small'
,
7
values
:
[
8
'sunny day at the beach'
,
9
'rainy afternoon in the city'
,
10
'snowy night in the mountains'
,
11
]
,
12
}
)
;
Retries
Both
embed
and
embedMany
accept an optional
maxRetries
parameter of type
number
that you can use to set the maximum number of retries for the embedding process.
It defaults to
2
retries (3 attempts in total). You can set it to
0
to disable retries.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
embed
}
from
'ai'
;
3
4
const
{
embedding
}
=
await
embed
(
{
5
model
:
'openai/text-embedding-3-small'
,
6
value
:
'sunny day at the beach'
,
7
maxRetries
:
0
,
// Disable retries
8
}
)
;
Abort Signals and Timeouts
Both
embed
and
embedMany
accept an optional
abortSignal
parameter of
type
AbortSignal
that you can use to abort the embedding process or set a timeout.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
embed
}
from
'ai'
;
3
4
const
{
embedding
}
=
await
embed
(
{
5
model
:
'openai/text-embedding-3-small'
,
6
value
:
'sunny day at the beach'
,
7
abortSignal
:
AbortSignal
.
timeout
(
1000
)
,
// Abort after 1 second
8
}
)
;
Custom Headers
Both
embed
and
embedMany
accept an optional
headers
parameter of type
Record<string, string>
that you can use to add custom headers to the embedding request.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
embed
}
from
'ai'
;
3
4
const
{
embedding
}
=
await
embed
(
{
5
model
:
'openai/text-embedding-3-small'
,
6
value
:
'sunny day at the beach'
,
7
headers
:
{
'X-Custom-Header'
:
'custom-value'
}
,
8
}
)
;
Response Information
Both
embed
and
embedMany
return response information that includes the raw provider response:
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
embed
}
from
'ai'
;
3
4
const
{
embedding
,
response
}
=
await
embed
(
{
5
model
:
'openai/text-embedding-3-small'
,
6
value
:
'sunny day at the beach'
,
7
}
)
;
8
9
console
.
log
(
response
)
;
// Raw provider response
Embedding Middleware
You can enhance embedding models, e.g. to set default values, using
wrapEmbeddingModel
and
EmbeddingModelV3Middleware
.
Here is an example that uses the built-in
defaultEmbeddingSettingsMiddleware
:
1
import
{
2
customProvider
,
3
defaultEmbeddingSettingsMiddleware
,
4
embed
,
5
wrapEmbeddingModel
,
6
gateway
,
7
}
from
'ai'
;
8
9
const
embeddingModelWithDefaults
=
wrapEmbeddingModel
(
{
10
model
:
gateway
.
embeddingModel
(
'google/gemini-embedding-001'
)
,
11
middleware
:
defaultEmbeddingSettingsMiddleware
(
{
12
settings
:
{
13
providerOptions
:
{
14
google
:
{
15
outputDimensionality
:
256
,
16
taskType
:
'CLASSIFICATION'
,
17
}
,
18
}
,
19
}
,
20
}
)
,
21
}
)
;
Embedding Providers & Models
Several providers offer embedding models:
Provider
Model
Embedding Dimensions
OpenAI
text-embedding-3-large
3072
OpenAI
text-embedding-3-small
1536
OpenAI
text-embedding-ada-002
1536
Google Generative AI
gemini-embedding-001
3072
Google Generative AI
text-embedding-004
768
Mistral
mistral-embed
1024
Cohere
embed-english-v3.0
1024
Cohere
embed-multilingual-v3.0
1024
Cohere
embed-english-light-v3.0
384
Cohere
embed-multilingual-light-v3.0
384
Cohere
embed-english-v2.0
4096
Cohere
embed-english-light-v2.0
1024
Cohere
embed-multilingual-v2.0
768
Amazon Bedrock
amazon.titan-embed-text-v1
1536
Amazon Bedrock
amazon.titan-embed-text-v2:0
1024
Previous
Settings
Next
Reranking
On this page
Embeddings
Embedding a Single Value
Embedding Many Values
Embedding Similarity
Token Usage
Settings
Provider Options
Parallel Requests
Retries
Abort Signals and Timeouts
Custom Headers
Response Information
Embedding Middleware
Embedding Providers & Models
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== RERANKING =====

AI SDK Core: Reranking
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Reranking
Copy markdown
Reranking
Reranking is a technique used to improve search relevance by reordering a set of documents based on their relevance to a query.
Unlike embedding-based similarity search, reranking models are specifically trained to understand the relationship between queries and documents,
often producing more accurate relevance scores.
Reranking Documents
The AI SDK provides the
rerank
function to rerank documents based on their relevance to a query.
You can use it with reranking models, e.g.
cohere.reranking('rerank-v3.5')
or
bedrock.reranking('cohere.rerank-v3-5:0')
.
1
import
{
rerank
}
from
'ai'
;
2
import
{
cohere
}
from
'@ai-sdk/cohere'
;
3
4
const
documents
=
[
5
'sunny day at the beach'
,
6
'rainy afternoon in the city'
,
7
'snowy night in the mountains'
,
8
]
;
9
10
const
{
ranking
}
=
await
rerank
(
{
11
model
:
cohere
.
reranking
(
'rerank-v3.5'
)
,
12
documents
,
13
query
:
'talk about rain'
,
14
topN
:
2
,
// Return top 2 most relevant documents
15
}
)
;
16
17
console
.
log
(
ranking
)
;
18
// [
19
//   { originalIndex: 1, score: 0.9, document: 'rainy afternoon in the city' },
20
//   { originalIndex: 0, score: 0.3, document: 'sunny day at the beach' }
21
// ]
Working with Object Documents
Reranking also supports structured documents (JSON objects), making it ideal for searching through databases, emails, or other structured content:
1
import
{
rerank
}
from
'ai'
;
2
import
{
cohere
}
from
'@ai-sdk/cohere'
;
3
4
const
documents
=
[
5
{
6
from
:
'Paul Doe'
,
7
subject
:
'Follow-up'
,
8
text
:
'We are happy to give you a discount of 20% on your next order.'
,
9
}
,
10
{
11
from
:
'John McGill'
,
12
subject
:
'Missing Info'
,
13
text
:
'Sorry, but here is the pricing information from Oracle: $5000/month'
,
14
}
,
15
]
;
16
17
const
{
ranking
,
rerankedDocuments
}
=
await
rerank
(
{
18
model
:
cohere
.
reranking
(
'rerank-v3.5'
)
,
19
documents
,
20
query
:
'Which pricing did we get from Oracle?'
,
21
topN
:
1
,
22
}
)
;
23
24
console
.
log
(
rerankedDocuments
[
0
]
)
;
25
// { from: 'John McGill', subject: 'Missing Info', text: '...' }
Understanding the Results
The
rerank
function returns a comprehensive result object:
1
import
{
cohere
}
from
'@ai-sdk/cohere'
;
2
import
{
rerank
}
from
'ai'
;
3
4
const
{
ranking
,
rerankedDocuments
,
originalDocuments
}
=
await
rerank
(
{
5
model
:
cohere
.
reranking
(
'rerank-v3.5'
)
,
6
documents
:
[
'sunny day at the beach'
,
'rainy afternoon in the city'
]
,
7
query
:
'talk about rain'
,
8
}
)
;
9
10
// ranking: sorted array of { originalIndex, score, document }
11
// rerankedDocuments: documents sorted by relevance (convenience property)
12
// originalDocuments: original documents array
Each item in the
ranking
array contains:
originalIndex
: Position in the original documents array
score
: Relevance score (typically 0-1, where higher is more relevant)
document
: The original document
Settings
Top-N Results
Use
topN
to limit the number of results returned. This is useful for retrieving only the most relevant documents:
1
import
{
cohere
}
from
'@ai-sdk/cohere'
;
2
import
{
rerank
}
from
'ai'
;
3
4
const
{
ranking
}
=
await
rerank
(
{
5
model
:
cohere
.
reranking
(
'rerank-v3.5'
)
,
6
documents
:
[
'doc1'
,
'doc2'
,
'doc3'
,
'doc4'
,
'doc5'
]
,
7
query
:
'relevant information'
,
8
topN
:
3
,
// Return only top 3 most relevant documents
9
}
)
;
Provider Options
Reranking model settings can be configured using
providerOptions
for provider-specific parameters:
1
import
{
cohere
}
from
'@ai-sdk/cohere'
;
2
import
{
rerank
}
from
'ai'
;
3
4
const
{
ranking
}
=
await
rerank
(
{
5
model
:
cohere
.
reranking
(
'rerank-v3.5'
)
,
6
documents
:
[
'sunny day at the beach'
,
'rainy afternoon in the city'
]
,
7
query
:
'talk about rain'
,
8
providerOptions
:
{
9
cohere
:
{
10
maxTokensPerDoc
:
1000
,
// Limit tokens per document
11
}
,
12
}
,
13
}
)
;
Retries
The
rerank
function accepts an optional
maxRetries
parameter of type
number
that you can use to set the maximum number of retries for the reranking process.
It defaults to
2
retries (3 attempts in total). You can set it to
0
to disable retries.
1
import
{
cohere
}
from
'@ai-sdk/cohere'
;
2
import
{
rerank
}
from
'ai'
;
3
4
const
{
ranking
}
=
await
rerank
(
{
5
model
:
cohere
.
reranking
(
'rerank-v3.5'
)
,
6
documents
:
[
'sunny day at the beach'
,
'rainy afternoon in the city'
]
,
7
query
:
'talk about rain'
,
8
maxRetries
:
0
,
// Disable retries
9
}
)
;
Abort Signals and Timeouts
The
rerank
function accepts an optional
abortSignal
parameter of
type
AbortSignal
that you can use to abort the reranking process or set a timeout.
1
import
{
cohere
}
from
'@ai-sdk/cohere'
;
2
import
{
rerank
}
from
'ai'
;
3
4
const
{
ranking
}
=
await
rerank
(
{
5
model
:
cohere
.
reranking
(
'rerank-v3.5'
)
,
6
documents
:
[
'sunny day at the beach'
,
'rainy afternoon in the city'
]
,
7
query
:
'talk about rain'
,
8
abortSignal
:
AbortSignal
.
timeout
(
5000
)
,
// Abort after 5 seconds
9
}
)
;
Custom Headers
The
rerank
function accepts an optional
headers
parameter of type
Record<string, string>
that you can use to add custom headers to the reranking request.
1
import
{
cohere
}
from
'@ai-sdk/cohere'
;
2
import
{
rerank
}
from
'ai'
;
3
4
const
{
ranking
}
=
await
rerank
(
{
5
model
:
cohere
.
reranking
(
'rerank-v3.5'
)
,
6
documents
:
[
'sunny day at the beach'
,
'rainy afternoon in the city'
]
,
7
query
:
'talk about rain'
,
8
headers
:
{
'X-Custom-Header'
:
'custom-value'
}
,
9
}
)
;
Response Information
The
rerank
function returns response information that includes the raw provider response:
1
import
{
cohere
}
from
'@ai-sdk/cohere'
;
2
import
{
rerank
}
from
'ai'
;
3
4
const
{
ranking
,
response
}
=
await
rerank
(
{
5
model
:
cohere
.
reranking
(
'rerank-v3.5'
)
,
6
documents
:
[
'sunny day at the beach'
,
'rainy afternoon in the city'
]
,
7
query
:
'talk about rain'
,
8
}
)
;
9
10
console
.
log
(
response
)
;
// { id, timestamp, modelId, headers, body }
Reranking Providers & Models
Several providers offer reranking models:
Provider
Model
Cohere
rerank-v3.5
Cohere
rerank-english-v3.0
Cohere
rerank-multilingual-v3.0
Amazon Bedrock
amazon.rerank-v1:0
Amazon Bedrock
cohere.rerank-v3-5:0
Together.ai
Salesforce/Llama-Rank-v1
Together.ai
mixedbread-ai/Mxbai-Rerank-Large-V2
Previous
Embeddings
Next
Image Generation
On this page
Reranking
Reranking Documents
Working with Object Documents
Understanding the Results
Settings
Top-N Results
Provider Options
Retries
Abort Signals and Timeouts
Custom Headers
Response Information
Reranking Providers & Models
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== IMAGE-GENERATION =====

AI SDK Core: Image Generation
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Image Generation
Copy markdown
Image Generation
The AI SDK provides the
generateImage
function to generate images based on a given prompt using an image model.
1
import
{
generateImage
}
from
'ai'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
4
const
{
image
}
=
await
generateImage
(
{
5
model
:
openai
.
image
(
'dall-e-3'
)
,
6
prompt
:
'Santa Claus driving a Cadillac'
,
7
}
)
;
You can access the image data using the
base64
or
uint8Array
properties:
1
const
base64
=
image
.
base64
;
// base64 image data
2
const
uint8Array
=
image
.
uint8Array
;
// Uint8Array image data
Settings
Size and Aspect Ratio
Depending on the model, you can either specify the size or the aspect ratio.
Size
The size is specified as a string in the format
{width}x{height}
.
Models only support a few sizes, and the supported sizes are different for each model and provider.
1
import
{
generateImage
}
from
'ai'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
4
const
{
image
}
=
await
generateImage
(
{
5
model
:
openai
.
image
(
'dall-e-3'
)
,
6
prompt
:
'Santa Claus driving a Cadillac'
,
7
size
:
'1024x1024'
,
8
}
)
;
Aspect Ratio
The aspect ratio is specified as a string in the format
{width}:{height}
.
Models only support a few aspect ratios, and the supported aspect ratios are different for each model and provider.
1
import
{
generateImage
}
from
'ai'
;
2
import
{
vertex
}
from
'@ai-sdk/google-vertex'
;
3
4
const
{
image
}
=
await
generateImage
(
{
5
model
:
vertex
.
image
(
'imagen-4.0-generate-001'
)
,
6
prompt
:
'Santa Claus driving a Cadillac'
,
7
aspectRatio
:
'16:9'
,
8
}
)
;
Generating Multiple Images
generateImage
also supports generating multiple images at once:
1
import
{
generateImage
}
from
'ai'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
4
const
{
images
}
=
await
generateImage
(
{
5
model
:
openai
.
image
(
'dall-e-2'
)
,
6
prompt
:
'Santa Claus driving a Cadillac'
,
7
n
:
4
,
// number of images to generate
8
}
)
;
generateImage
will automatically call the model as often as needed (in
parallel) to generate the requested number of images.
Each image model has an internal limit on how many images it can generate in a single API call. The AI SDK manages this automatically by batching requests appropriately when you request multiple images using the
n
parameter. By default, the SDK uses provider-documented limits (for example, DALL-E 3 can only generate 1 image per call, while DALL-E 2 supports up to 10).
If needed, you can override this behavior using the
maxImagesPerCall
setting when generating your image. This is particularly useful when working with new or custom models where the default batch size might not be optimal:
1
const
{
images
}
=
await
generateImage
(
{
2
model
:
openai
.
image
(
'dall-e-2'
)
,
3
prompt
:
'Santa Claus driving a Cadillac'
,
4
maxImagesPerCall
:
5
,
// Override the default batch size
5
n
:
10
,
// Will make 2 calls of 5 images each
6
}
)
;
Providing a Seed
You can provide a seed to the
generateImage
function to control the output of the image generation process.
If supported by the model, the same seed will always produce the same image.
1
import
{
generateImage
}
from
'ai'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
4
const
{
image
}
=
await
generateImage
(
{
5
model
:
openai
.
image
(
'dall-e-3'
)
,
6
prompt
:
'Santa Claus driving a Cadillac'
,
7
seed
:
1234567890
,
8
}
)
;
Provider-specific Settings
Image models often have provider- or even model-specific settings.
You can pass such settings to the
generateImage
function
using the
providerOptions
parameter. The options for the provider
(
openai
in the example below) become request body properties.
1
import
{
generateImage
}
from
'ai'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
4
const
{
image
}
=
await
generateImage
(
{
5
model
:
openai
.
image
(
'dall-e-3'
)
,
6
prompt
:
'Santa Claus driving a Cadillac'
,
7
size
:
'1024x1024'
,
8
providerOptions
:
{
9
openai
:
{
style
:
'vivid'
,
quality
:
'hd'
}
,
10
}
,
11
}
)
;
Abort Signals and Timeouts
generateImage
accepts an optional
abortSignal
parameter of
type
AbortSignal
that you can use to abort the image generation process or set a timeout.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
generateImage
}
from
'ai'
;
3
4
const
{
image
}
=
await
generateImage
(
{
5
model
:
openai
.
image
(
'dall-e-3'
)
,
6
prompt
:
'Santa Claus driving a Cadillac'
,
7
abortSignal
:
AbortSignal
.
timeout
(
1000
)
,
// Abort after 1 second
8
}
)
;
Custom Headers
generateImage
accepts an optional
headers
parameter of type
Record<string, string>
that you can use to add custom headers to the image generation request.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
generateImage
}
from
'ai'
;
3
4
const
{
image
}
=
await
generateImage
(
{
5
model
:
openai
.
image
(
'dall-e-3'
)
,
6
prompt
:
'Santa Claus driving a Cadillac'
,
7
headers
:
{
'X-Custom-Header'
:
'custom-value'
}
,
8
}
)
;
Warnings
If the model returns warnings, e.g. for unsupported parameters, they will be available in the
warnings
property of the response.
1
const
{
image
,
warnings
}
=
await
generateImage
(
{
2
model
:
openai
.
image
(
'dall-e-3'
)
,
3
prompt
:
'Santa Claus driving a Cadillac'
,
4
}
)
;
Additional provider-specific meta data
Some providers expose additional meta data for the result overall or per image.
1
const
prompt
=
'Santa Claus driving a Cadillac'
;
2
3
const
{
image
,
providerMetadata
}
=
await
generateImage
(
{
4
model
:
openai
.
image
(
'dall-e-3'
)
,
5
prompt
,
6
}
)
;
7
8
const
revisedPrompt
=
providerMetadata
.
openai
.
images
[
0
]
?.
revisedPrompt
;
9
10
console
.
log
(
{
11
prompt
,
12
revisedPrompt
,
13
}
)
;
The outer key of the returned
providerMetadata
is the provider name. The inner values are the metadata. An
images
key is always present in the metadata and is an array with the same length as the top level
images
key.
Error Handling
When
generateImage
cannot generate a valid image, it throws a
AI_NoImageGeneratedError
.
This error occurs when the AI provider fails to generate an image. It can arise due to the following reasons:
The model failed to generate a response
The model generated a response that could not be parsed
The error preserves the following information to help you log the issue:
responses
: Metadata about the image model responses, including timestamp, model, and headers.
cause
: The cause of the error. You can use this for more detailed error handling
1
import
{
generateImage
,
NoImageGeneratedError
}
from
'ai'
;
2
3
try
{
4
await
generateImage
(
{
model
,
prompt
}
)
;
5
}
catch
(
error
)
{
6
if
(
NoImageGeneratedError
.
isInstance
(
error
)
)
{
7
console
.
log
(
'NoImageGeneratedError'
)
;
8
console
.
log
(
'Cause:'
,
error
.
cause
)
;
9
console
.
log
(
'Responses:'
,
error
.
responses
)
;
10
}
11
}
Image Middleware
You can enhance image models, e.g. to set default values or implement logging, using
wrapImageModel
and
ImageModelV3Middleware
.
Here is an example that sets a default size when none is provided:
1
import
{
generateImage
,
wrapImageModel
}
from
'ai'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
4
const
model
=
wrapImageModel
(
{
5
model
:
openai
.
image
(
'gpt-image-1'
)
,
6
middleware
:
{
7
specificationVersion
:
'v3'
,
8
transformParams
:
async
(
{
params
}
)
=>
(
{
9
...
params
,
10
size
:
params
.
size
??
'1024x1024'
,
11
}
)
,
12
}
,
13
}
)
;
14
15
const
{
image
}
=
await
generateImage
(
{
16
model
,
17
prompt
:
'Santa Claus driving a Cadillac'
,
18
}
)
;
Generating Images with Language Models
Some language models such as Google
gemini-2.5-flash-image-preview
support multi-modal outputs including images.
With such models, you can access the generated images using the
files
property of the response.
1
import
{
google
}
from
'@ai-sdk/google'
;
2
import
{
generateText
}
from
'ai'
;
3
4
const
result
=
await
generateText
(
{
5
model
:
google
(
'gemini-2.5-flash-image-preview'
)
,
6
prompt
:
'Generate an image of a comic cat'
,
7
}
)
;
8
9
for
(
const
file
of
result
.
files
)
{
10
if
(
file
.
mediaType
.
startsWith
(
'image/'
)
)
{
11
// The file object provides multiple data formats:
12
// Access images as base64 string, Uint8Array binary data, or check type
13
// - file.base64: string (data URL format)
14
// - file.uint8Array: Uint8Array (binary data)
15
// - file.mediaType: string (e.g. "image/png")
16
}
17
}
Image Models
Provider
Model
Support sizes (
width x height
) or aspect ratios (
width : height
)
xAI Grok
grok-2-image
1024x768 (default)
OpenAI
gpt-image-1
1024x1024, 1536x1024, 1024x1536
OpenAI
dall-e-3
1024x1024, 1792x1024, 1024x1792
OpenAI
dall-e-2
256x256, 512x512, 1024x1024
Amazon Bedrock
amazon.nova-canvas-v1:0
320-4096 (multiples of 16), 1:4 to 4:1, max 4.2M pixels
Fal
fal-ai/flux/dev
1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9
Fal
fal-ai/flux-lora
1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9
Fal
fal-ai/fast-sdxl
1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9
Fal
fal-ai/flux-pro/v1.1-ultra
1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9
Fal
fal-ai/ideogram/v2
1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9
Fal
fal-ai/recraft-v3
1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9
Fal
fal-ai/stable-diffusion-3.5-large
1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9
Fal
fal-ai/hyper-sdxl
1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9
DeepInfra
stabilityai/sd3.5
1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21
DeepInfra
black-forest-labs/FLUX-1.1-pro
256-1440 (multiples of 32)
DeepInfra
black-forest-labs/FLUX-1-schnell
256-1440 (multiples of 32)
DeepInfra
black-forest-labs/FLUX-1-dev
256-1440 (multiples of 32)
DeepInfra
black-forest-labs/FLUX-pro
256-1440 (multiples of 32)
DeepInfra
stabilityai/sd3.5-medium
1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21
DeepInfra
stabilityai/sdxl-turbo
1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21
Replicate
black-forest-labs/flux-schnell
1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9
Replicate
recraft-ai/recraft-v3
1024x1024, 1365x1024, 1024x1365, 1536x1024, 1024x1536, 1820x1024, 1024x1820, 1024x2048, 2048x1024, 1434x1024, 1024x1434, 1024x1280, 1280x1024, 1024x1707, 1707x1024
Google
imagen-4.0-generate-001
1:1, 3:4, 4:3, 9:16, 16:9
Google
imagen-4.0-fast-generate-001
1:1, 3:4, 4:3, 9:16, 16:9
Google
imagen-4.0-ultra-generate-001
1:1, 3:4, 4:3, 9:16, 16:9
Google Vertex
imagen-4.0-generate-001
1:1, 3:4, 4:3, 9:16, 16:9
Google Vertex
imagen-4.0-fast-generate-001
1:1, 3:4, 4:3, 9:16, 16:9
Google Vertex
imagen-4.0-ultra-generate-001
1:1, 3:4, 4:3, 9:16, 16:9
Google Vertex
imagen-3.0-fast-generate-001
1:1, 3:4, 4:3, 9:16, 16:9
Fireworks
accounts/fireworks/models/flux-1-dev-fp8
1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9
Fireworks
accounts/fireworks/models/flux-1-schnell-fp8
1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9
Fireworks
accounts/fireworks/models/playground-v2-5-1024px-aesthetic
640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640
Fireworks
accounts/fireworks/models/japanese-stable-diffusion-xl
640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640
Fireworks
accounts/fireworks/models/playground-v2-1024px-aesthetic
640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640
Fireworks
accounts/fireworks/models/SSD-1B
640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640
Fireworks
accounts/fireworks/models/stable-diffusion-xl-1024-v1-0
640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640
Luma
photon-1
1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9
Luma
photon-flash-1
1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9
Together.ai
stabilityai/stable-diffusion-xl-base-1.0
512x512, 768x768, 1024x1024
Together.ai
black-forest-labs/FLUX.1-dev
512x512, 768x768, 1024x1024
Together.ai
black-forest-labs/FLUX.1-dev-lora
512x512, 768x768, 1024x1024
Together.ai
black-forest-labs/FLUX.1-schnell
512x512, 768x768, 1024x1024
Together.ai
black-forest-labs/FLUX.1-canny
512x512, 768x768, 1024x1024
Together.ai
black-forest-labs/FLUX.1-depth
512x512, 768x768, 1024x1024
Together.ai
black-forest-labs/FLUX.1-redux
512x512, 768x768, 1024x1024
Together.ai
black-forest-labs/FLUX.1.1-pro
512x512, 768x768, 1024x1024
Together.ai
black-forest-labs/FLUX.1-pro
512x512, 768x768, 1024x1024
Together.ai
black-forest-labs/FLUX.1-schnell-Free
512x512, 768x768, 1024x1024
Black Forest Labs
flux-kontext-pro
From 3:7 (portrait) to 7:3 (landscape)
Black Forest Labs
flux-kontext-max
From 3:7 (portrait) to 7:3 (landscape)
Black Forest Labs
flux-pro-1.1-ultra
From 3:7 (portrait) to 7:3 (landscape)
Black Forest Labs
flux-pro-1.1
From 3:7 (portrait) to 7:3 (landscape)
Black Forest Labs
flux-pro-1.0-fill
From 3:7 (portrait) to 7:3 (landscape)
Above are a small subset of the image models supported by the AI SDK providers. For more, see the respective provider documentation.
Previous
Reranking
Next
Transcription
On this page
Image Generation
Settings
Size and Aspect Ratio
Size
Aspect Ratio
Generating Multiple Images
Providing a Seed
Provider-specific Settings
Abort Signals and Timeouts
Custom Headers
Warnings
Additional provider-specific meta data
Error Handling
Image Middleware
Generating Images with Language Models
Image Models
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== TRANSCRIPTION =====

AI SDK Core: Transcription
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Transcription
Copy markdown
Transcription
Transcription is an experimental feature.
The AI SDK provides the
transcribe
function to transcribe audio using a transcription model.
1
import
{
experimental_transcribe
as
transcribe
}
from
'ai'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
import
{
readFile
}
from
'fs/promises'
;
4
5
const
transcript
=
await
transcribe
(
{
6
model
:
openai
.
transcription
(
'whisper-1'
)
,
7
audio
:
await
readFile
(
'audio.mp3'
)
,
8
}
)
;
The
audio
property can be a
Uint8Array
,
ArrayBuffer
,
Buffer
,
string
(base64 encoded audio data), or a
URL
.
To access the generated transcript:
1
const
text
=
transcript
.
text
;
// transcript text e.g. "Hello, world!"
2
const
segments
=
transcript
.
segments
;
// array of segments with start and end times, if available
3
const
language
=
transcript
.
language
;
// language of the transcript e.g. "en", if available
4
const
durationInSeconds
=
transcript
.
durationInSeconds
;
// duration of the transcript in seconds, if available
Settings
Provider-Specific settings
Transcription models often have provider or model-specific settings which you can set using the
providerOptions
parameter.
1
import
{
experimental_transcribe
as
transcribe
}
from
'ai'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
import
{
readFile
}
from
'fs/promises'
;
4
5
const
transcript
=
await
transcribe
(
{
6
model
:
openai
.
transcription
(
'whisper-1'
)
,
7
audio
:
await
readFile
(
'audio.mp3'
)
,
8
providerOptions
:
{
9
openai
:
{
10
timestampGranularities
:
[
'word'
]
,
11
}
,
12
}
,
13
}
)
;
Abort Signals and Timeouts
transcribe
accepts an optional
abortSignal
parameter of
type
AbortSignal
that you can use to abort the transcription process or set a timeout.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
experimental_transcribe
as
transcribe
}
from
'ai'
;
3
import
{
readFile
}
from
'fs/promises'
;
4
5
const
transcript
=
await
transcribe
(
{
6
model
:
openai
.
transcription
(
'whisper-1'
)
,
7
audio
:
await
readFile
(
'audio.mp3'
)
,
8
abortSignal
:
AbortSignal
.
timeout
(
1000
)
,
// Abort after 1 second
9
}
)
;
Custom Headers
transcribe
accepts an optional
headers
parameter of type
Record<string, string>
that you can use to add custom headers to the transcription request.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
experimental_transcribe
as
transcribe
}
from
'ai'
;
3
import
{
readFile
}
from
'fs/promises'
;
4
5
const
transcript
=
await
transcribe
(
{
6
model
:
openai
.
transcription
(
'whisper-1'
)
,
7
audio
:
await
readFile
(
'audio.mp3'
)
,
8
headers
:
{
'X-Custom-Header'
:
'custom-value'
}
,
9
}
)
;
Warnings
Warnings (e.g. unsupported parameters) are available on the
warnings
property.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
experimental_transcribe
as
transcribe
}
from
'ai'
;
3
import
{
readFile
}
from
'fs/promises'
;
4
5
const
transcript
=
await
transcribe
(
{
6
model
:
openai
.
transcription
(
'whisper-1'
)
,
7
audio
:
await
readFile
(
'audio.mp3'
)
,
8
}
)
;
9
10
const
warnings
=
transcript
.
warnings
;
Error Handling
When
transcribe
cannot generate a valid transcript, it throws a
AI_NoTranscriptGeneratedError
.
This error can arise for any the following reasons:
The model failed to generate a response
The model generated a response that could not be parsed
The error preserves the following information to help you log the issue:
responses
: Metadata about the transcription model responses, including timestamp, model, and headers.
cause
: The cause of the error. You can use this for more detailed error handling.
1
import
{
2
experimental_transcribe
as
transcribe
,
3
NoTranscriptGeneratedError
,
4
}
from
'ai'
;
5
import
{
openai
}
from
'@ai-sdk/openai'
;
6
import
{
readFile
}
from
'fs/promises'
;
7
8
try
{
9
await
transcribe
(
{
10
model
:
openai
.
transcription
(
'whisper-1'
)
,
11
audio
:
await
readFile
(
'audio.mp3'
)
,
12
}
)
;
13
}
catch
(
error
)
{
14
if
(
NoTranscriptGeneratedError
.
isInstance
(
error
)
)
{
15
console
.
log
(
'NoTranscriptGeneratedError'
)
;
16
console
.
log
(
'Cause:'
,
error
.
cause
)
;
17
console
.
log
(
'Responses:'
,
error
.
responses
)
;
18
}
19
}
Transcription Models
Provider
Model
OpenAI
whisper-1
OpenAI
gpt-4o-transcribe
OpenAI
gpt-4o-mini-transcribe
ElevenLabs
scribe_v1
ElevenLabs
scribe_v1_experimental
Groq
whisper-large-v3-turbo
Groq
whisper-large-v3
Azure OpenAI
whisper-1
Azure OpenAI
gpt-4o-transcribe
Azure OpenAI
gpt-4o-mini-transcribe
Rev.ai
machine
Rev.ai
low_cost
Rev.ai
fusion
Deepgram
base
(+ variants)
Deepgram
enhanced
(+ variants)
Deepgram
nova
(+ variants)
Deepgram
nova-2
(+ variants)
Deepgram
nova-3
(+ variants)
Gladia
default
AssemblyAI
best
AssemblyAI
nano
Fal
whisper
Fal
wizper
Above are a small subset of the transcription models supported by the AI SDK providers. For more, see the respective provider documentation.
Previous
Image Generation
Next
Speech
On this page
Transcription
Settings
Provider-Specific settings
Abort Signals and Timeouts
Custom Headers
Warnings
Error Handling
Transcription Models
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== SPEECH =====

AI SDK Core: Speech
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Speech
Copy markdown
Speech
Speech is an experimental feature.
The AI SDK provides the
generateSpeech
function to generate speech from text using a speech model.
1
import
{
experimental_generateSpeech
as
generateSpeech
}
from
'ai'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
4
const
audio
=
await
generateSpeech
(
{
5
model
:
openai
.
speech
(
'tts-1'
)
,
6
text
:
'Hello, world!'
,
7
voice
:
'alloy'
,
8
}
)
;
Language Setting
You can specify the language for speech generation (provider support varies):
1
import
{
experimental_generateSpeech
as
generateSpeech
}
from
'ai'
;
2
import
{
lmnt
}
from
'@ai-sdk/lmnt'
;
3
4
const
audio
=
await
generateSpeech
(
{
5
model
:
lmnt
.
speech
(
'aurora'
)
,
6
text
:
'Hola, mundo!'
,
7
language
:
'es'
,
// Spanish
8
}
)
;
To access the generated audio:
1
const
audio
=
audio
.
audioData
;
// audio data e.g. Uint8Array
Settings
Provider-Specific settings
You can set model-specific settings with the
providerOptions
parameter.
1
import
{
experimental_generateSpeech
as
generateSpeech
}
from
'ai'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
4
const
audio
=
await
generateSpeech
(
{
5
model
:
openai
.
speech
(
'tts-1'
)
,
6
text
:
'Hello, world!'
,
7
providerOptions
:
{
8
openai
:
{
9
// ...
10
}
,
11
}
,
12
}
)
;
Abort Signals and Timeouts
generateSpeech
accepts an optional
abortSignal
parameter of
type
AbortSignal
that you can use to abort the speech generation process or set a timeout.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
experimental_generateSpeech
as
generateSpeech
}
from
'ai'
;
3
4
const
audio
=
await
generateSpeech
(
{
5
model
:
openai
.
speech
(
'tts-1'
)
,
6
text
:
'Hello, world!'
,
7
abortSignal
:
AbortSignal
.
timeout
(
1000
)
,
// Abort after 1 second
8
}
)
;
Custom Headers
generateSpeech
accepts an optional
headers
parameter of type
Record<string, string>
that you can use to add custom headers to the speech generation request.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
experimental_generateSpeech
as
generateSpeech
}
from
'ai'
;
3
4
const
audio
=
await
generateSpeech
(
{
5
model
:
openai
.
speech
(
'tts-1'
)
,
6
text
:
'Hello, world!'
,
7
headers
:
{
'X-Custom-Header'
:
'custom-value'
}
,
8
}
)
;
Warnings
Warnings (e.g. unsupported parameters) are available on the
warnings
property.
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
import
{
experimental_generateSpeech
as
generateSpeech
}
from
'ai'
;
3
4
const
audio
=
await
generateSpeech
(
{
5
model
:
openai
.
speech
(
'tts-1'
)
,
6
text
:
'Hello, world!'
,
7
}
)
;
8
9
const
warnings
=
audio
.
warnings
;
Error Handling
When
generateSpeech
cannot generate a valid audio, it throws a
AI_NoSpeechGeneratedError
.
This error can arise for any the following reasons:
The model failed to generate a response
The model generated a response that could not be parsed
The error preserves the following information to help you log the issue:
responses
: Metadata about the speech model responses, including timestamp, model, and headers.
cause
: The cause of the error. You can use this for more detailed error handling.
1
import
{
2
experimental_generateSpeech
as
generateSpeech
,
3
NoSpeechGeneratedError
,
4
}
from
'ai'
;
5
import
{
openai
}
from
'@ai-sdk/openai'
;
6
7
try
{
8
await
generateSpeech
(
{
9
model
:
openai
.
speech
(
'tts-1'
)
,
10
text
:
'Hello, world!'
,
11
}
)
;
12
}
catch
(
error
)
{
13
if
(
NoSpeechGeneratedError
.
isInstance
(
error
)
)
{
14
console
.
log
(
'AI_NoSpeechGeneratedError'
)
;
15
console
.
log
(
'Cause:'
,
error
.
cause
)
;
16
console
.
log
(
'Responses:'
,
error
.
responses
)
;
17
}
18
}
Speech Models
Provider
Model
OpenAI
tts-1
OpenAI
tts-1-hd
OpenAI
gpt-4o-mini-tts
ElevenLabs
eleven_v3
ElevenLabs
eleven_multilingual_v2
ElevenLabs
eleven_flash_v2_5
ElevenLabs
eleven_flash_v2
ElevenLabs
eleven_turbo_v2_5
ElevenLabs
eleven_turbo_v2
LMNT
aurora
LMNT
blizzard
Hume
default
Above are a small subset of the speech models supported by the AI SDK providers. For more, see the respective provider documentation.
Previous
Transcription
Next
Language Model Middleware
On this page
Speech
Language Setting
Settings
Provider-Specific settings
Abort Signals and Timeouts
Custom Headers
Warnings
Error Handling
Speech Models
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== MIDDLEWARE =====

AI SDK Core: Language Model Middleware
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Language Model Middleware
Copy markdown
Language Model Middleware
Language model middleware is a way to enhance the behavior of language models
by intercepting and modifying the calls to the language model.
It can be used to add features like guardrails, RAG, caching, and logging
in a language model agnostic way. Such middleware can be developed and
distributed independently from the language models that they are applied to.
Using Language Model Middleware
You can use language model middleware with the
wrapLanguageModel
function.
It takes a language model and a language model middleware and returns a new
language model that incorporates the middleware.
1
import
{
wrapLanguageModel
}
from
'ai'
;
2
3
const
wrappedLanguageModel
=
wrapLanguageModel
(
{
4
model
:
yourModel
,
5
middleware
:
yourLanguageModelMiddleware
,
6
}
)
;
The wrapped language model can be used just like any other language model, e.g. in
streamText
:
1
const
result
=
streamText
(
{
2
model
:
wrappedLanguageModel
,
3
prompt
:
'What cities are in the United States?'
,
4
}
)
;
Multiple middlewares
You can provide multiple middlewares to the
wrapLanguageModel
function.
The middlewares will be applied in the order they are provided.
1
const
wrappedLanguageModel
=
wrapLanguageModel
(
{
2
model
:
yourModel
,
3
middleware
:
[
firstMiddleware
,
secondMiddleware
]
,
4
}
)
;
5
6
// applied as: firstMiddleware(secondMiddleware(yourModel))
Built-in Middleware
The AI SDK comes with several built-in middlewares that you can use to configure language models:
extractReasoningMiddleware
: Extracts reasoning information from the generated text and exposes it as a
reasoning
property on the result.
simulateStreamingMiddleware
: Simulates streaming behavior with responses from non-streaming language models.
defaultSettingsMiddleware
: Applies default settings to a language model.
addToolInputExamplesMiddleware
: Adds tool input examples to tool descriptions for providers that don't natively support the
inputExamples
property.
Extract Reasoning
Some providers and models expose reasoning information in the generated text using special tags,
e.g. <think> and </think>.
The
extractReasoningMiddleware
function can be used to extract this reasoning information and expose it as a
reasoning
property on the result.
1
import
{
wrapLanguageModel
,
extractReasoningMiddleware
}
from
'ai'
;
2
3
const
model
=
wrapLanguageModel
(
{
4
model
:
yourModel
,
5
middleware
:
extractReasoningMiddleware
(
{
tagName
:
'think'
}
)
,
6
}
)
;
You can then use that enhanced model in functions like
generateText
and
streamText
.
The
extractReasoningMiddleware
function also includes a
startWithReasoning
option.
When set to
true
, the reasoning tag will be prepended to the generated text.
This is useful for models that do not include the reasoning tag at the beginning of the response.
For more details, see the
DeepSeek R1 guide
.
Simulate Streaming
The
simulateStreamingMiddleware
function can be used to simulate streaming behavior with responses from non-streaming language models.
This is useful when you want to maintain a consistent streaming interface even when using models that only provide complete responses.
1
import
{
wrapLanguageModel
,
simulateStreamingMiddleware
}
from
'ai'
;
2
3
const
model
=
wrapLanguageModel
(
{
4
model
:
yourModel
,
5
middleware
:
simulateStreamingMiddleware
(
)
,
6
}
)
;
Default Settings
The
defaultSettingsMiddleware
function can be used to apply default settings to a language model.
1
import
{
wrapLanguageModel
,
defaultSettingsMiddleware
}
from
'ai'
;
2
3
const
model
=
wrapLanguageModel
(
{
4
model
:
yourModel
,
5
middleware
:
defaultSettingsMiddleware
(
{
6
settings
:
{
7
temperature
:
0.5
,
8
maxOutputTokens
:
800
,
9
providerOptions
:
{
openai
:
{
store
:
false
}
}
,
10
}
,
11
}
)
,
12
}
)
;
Add Tool Input Examples
The
addToolInputExamplesMiddleware
function adds tool input examples to tool descriptions.
This is useful for providers that don't natively support the
inputExamples
property on tools.
The middleware serializes the examples into the tool's description text so models can still benefit from seeing example inputs.
1
import
{
wrapLanguageModel
,
addToolInputExamplesMiddleware
}
from
'ai'
;
2
3
const
model
=
wrapLanguageModel
(
{
4
model
:
yourModel
,
5
middleware
:
addToolInputExamplesMiddleware
(
{
6
examplesPrefix
:
'Input Examples:'
,
7
}
)
,
8
}
)
;
When you define a tool with
inputExamples
, the middleware will append them to the tool's description:
1
import
{
generateText
,
tool
}
from
'ai'
;
2
import
{
z
}
from
'zod'
;
3
4
const
result
=
await
generateText
(
{
5
model
,
// wrapped model from above
6
tools
:
{
7
weather
:
tool
(
{
8
description
:
'Get the weather in a location'
,
9
inputSchema
:
z
.
object
(
{
10
location
:
z
.
string
(
)
,
11
}
)
,
12
inputExamples
:
[
13
{
input
:
{
location
:
'San Francisco'
}
}
,
14
{
input
:
{
location
:
'London'
}
}
,
15
]
,
16
}
)
,
17
}
,
18
prompt
:
'What is the weather in Tokyo?'
,
19
}
)
;
The tool description will be transformed to:
1
Get the weather
in
a location
2
3
Input Examples
:
4
{
"location"
:
"San Francisco"
}
5
{
"location"
:
"London"
}
Options
examplesPrefix
(required): A prefix text to prepend before the examples.
formatExample
(optional): A custom formatter function for each example. Receives the example object and its index. Default:
JSON.stringify(example.input)
.
removeInputExamples
(optional): Whether to remove the
inputExamples
property from the tool after adding them to the description. Default:
true
.
1
const
model
=
wrapLanguageModel
(
{
2
model
:
yourModel
,
3
middleware
:
addToolInputExamplesMiddleware
(
{
4
examplesPrefix
:
'Input Examples:'
,
5
formatExample
:
(
example
,
index
)
=>
6
`
${
index
+
1
}
.
${
JSON
.
stringify
(
example
.
input
)
}
`
,
7
removeInputExamples
:
true
,
8
}
)
,
9
}
)
;
Community Middleware
The AI SDK provides a Language Model Middleware specification. Community members can develop middleware that adheres to this specification, making it compatible with the AI SDK ecosystem.
Here are some community middlewares that you can explore:
Custom tool call parser
The
Custom tool call parser
middleware extends tool call capabilities to models that don't natively support the OpenAI-style
tools
parameter. This includes many self-hosted and third-party models that lack native function calling features.
Using this middleware on models that support native function calls may result
in unintended performance degradation, so check whether your model supports
native function calls before deciding to use it.
This middleware enables function calling capabilities by converting function schemas into prompt instructions and parsing the model's responses into structured function calls. It works by transforming the JSON function definitions into natural language instructions the model can understand, then analyzing the generated text to extract function call attempts. This approach allows developers to use the same function calling API across different model providers, even with models that don't natively support the OpenAI-style function calling format, providing a consistent function calling experience regardless of the underlying model implementation.
The
@ai-sdk-tool/parser
package offers three middleware variants:
createToolMiddleware
: A flexible function for creating custom tool call middleware tailored to specific models
hermesToolMiddleware
: Ready-to-use middleware for Hermes & Qwen format function calls
gemmaToolMiddleware
: Pre-configured middleware for Gemma 3 model series function call format
Here's how you can enable function calls with Gemma models that don't support them natively:
1
import
{
wrapLanguageModel
}
from
'ai'
;
2
import
{
gemmaToolMiddleware
}
from
'@ai-sdk-tool/parser'
;
3
4
const
model
=
wrapLanguageModel
(
{
5
model
:
openrouter
(
'google/gemma-3-27b-it'
)
,
6
middleware
:
gemmaToolMiddleware
,
7
}
)
;
Find more examples at this
link
.
Implementing Language Model Middleware
Implementing language model middleware is advanced functionality and requires
a solid understanding of the
language model
specification
.
You can implement any of the following three function to modify the behavior of the language model:
transformParams
: Transforms the parameters before they are passed to the language model, for both
doGenerate
and
doStream
.
wrapGenerate
: Wraps the
doGenerate
method of the
language model
.
You can modify the parameters, call the language model, and modify the result.
wrapStream
: Wraps the
doStream
method of the
language model
.
You can modify the parameters, call the language model, and modify the result.
Here are some examples of how to implement language model middleware:
Examples
These examples are not meant to be used in production. They are just to show
how you can use middleware to enhance the behavior of language models.
Logging
This example shows how to log the parameters and generated text of a language model call.
1
import
type
{
2
LanguageModelV3Middleware
,
3
LanguageModelV3StreamPart
,
4
}
from
'@ai-sdk/provider'
;
5
6
export
const
yourLogMiddleware
:
LanguageModelV3Middleware
=
{
7
wrapGenerate
:
async
(
{
doGenerate
,
params
}
)
=>
{
8
console
.
log
(
'doGenerate called'
)
;
9
console
.
log
(
`
params:
${
JSON
.
stringify
(
params
,
null
,
2
)
}
`
)
;
10
11
const
result
=
await
doGenerate
(
)
;
12
13
console
.
log
(
'doGenerate finished'
)
;
14
console
.
log
(
`
generated text:
${
result
.
text
}
`
)
;
15
16
return
result
;
17
}
,
18
19
wrapStream
:
async
(
{
doStream
,
params
}
)
=>
{
20
console
.
log
(
'doStream called'
)
;
21
console
.
log
(
`
params:
${
JSON
.
stringify
(
params
,
null
,
2
)
}
`
)
;
22
23
const
{
stream
,
...
rest
}
=
await
doStream
(
)
;
24
25
let
generatedText
=
''
;
26
const
textBlocks
=
new
Map
<
string
,
string
>
(
)
;
27
28
const
transformStream
=
new
TransformStream
<
29
LanguageModelV3StreamPart
,
30
LanguageModelV3StreamPart
31
>
(
{
32
transform
(
chunk
,
controller
)
{
33
switch
(
chunk
.
type
)
{
34
case
'text-start'
:
{
35
textBlocks
.
set
(
chunk
.
id
,
''
)
;
36
break
;
37
}
38
case
'text-delta'
:
{
39
const
existing
=
textBlocks
.
get
(
chunk
.
id
)
||
''
;
40
textBlocks
.
set
(
chunk
.
id
,
existing
+
chunk
.
delta
)
;
41
generatedText
+=
chunk
.
delta
;
42
break
;
43
}
44
case
'text-end'
:
{
45
console
.
log
(
46
`
Text block
${
chunk
.
id
}
completed:
`
,
47
textBlocks
.
get
(
chunk
.
id
)
,
48
)
;
49
break
;
50
}
51
}
52
53
controller
.
enqueue
(
chunk
)
;
54
}
,
55
56
flush
(
)
{
57
console
.
log
(
'doStream finished'
)
;
58
console
.
log
(
`
generated text:
${
generatedText
}
`
)
;
59
}
,
60
}
)
;
61
62
return
{
63
stream
:
stream
.
pipeThrough
(
transformStream
)
,
64
...
rest
,
65
}
;
66
}
,
67
}
;
Caching
This example shows how to build a simple cache for the generated text of a language model call.
1
import
type
{
LanguageModelV3Middleware
}
from
'@ai-sdk/provider'
;
2
3
const
cache
=
new
Map
<
string
,
any
>
(
)
;
4
5
export
const
yourCacheMiddleware
:
LanguageModelV3Middleware
=
{
6
wrapGenerate
:
async
(
{
doGenerate
,
params
}
)
=>
{
7
const
cacheKey
=
JSON
.
stringify
(
params
)
;
8
9
if
(
cache
.
has
(
cacheKey
)
)
{
10
return
cache
.
get
(
cacheKey
)
;
11
}
12
13
const
result
=
await
doGenerate
(
)
;
14
15
cache
.
set
(
cacheKey
,
result
)
;
16
17
return
result
;
18
}
,
19
20
// here you would implement the caching logic for streaming
21
}
;
Retrieval Augmented Generation (RAG)
This example shows how to use RAG as middleware.
Helper functions like
getLastUserMessageText
and
findSources
are not part
of the AI SDK. They are just used in this example to illustrate the concept of
RAG.
1
import
type
{
LanguageModelV3Middleware
}
from
'@ai-sdk/provider'
;
2
3
export
const
yourRagMiddleware
:
LanguageModelV3Middleware
=
{
4
transformParams
:
async
(
{
params
}
)
=>
{
5
const
lastUserMessageText
=
getLastUserMessageText
(
{
6
prompt
:
params
.
prompt
,
7
}
)
;
8
9
if
(
lastUserMessageText
==
null
)
{
10
return
params
;
// do not use RAG (send unmodified parameters)
11
}
12
13
const
instruction
=
14
'Use the following information to answer the question:\n'
+
15
findSources
(
{
text
:
lastUserMessageText
}
)
16
.
map
(
chunk
=>
JSON
.
stringify
(
chunk
)
)
17
.
join
(
'\n'
)
;
18
19
return
addToLastUserMessage
(
{
params
,
text
:
instruction
}
)
;
20
}
,
21
}
;
Guardrails
Guard rails are a way to ensure that the generated text of a language model call
is safe and appropriate. This example shows how to use guardrails as middleware.
1
import
type
{
LanguageModelV3Middleware
}
from
'@ai-sdk/provider'
;
2
3
export
const
yourGuardrailMiddleware
:
LanguageModelV3Middleware
=
{
4
wrapGenerate
:
async
(
{
doGenerate
}
)
=>
{
5
const
{
text
,
...
rest
}
=
await
doGenerate
(
)
;
6
7
// filtering approach, e.g. for PII or other sensitive information:
8
const
cleanedText
=
text
?.
replace
(
/
badword
/
g
,
'<REDACTED>'
)
;
9
10
return
{
text
:
cleanedText
,
...
rest
}
;
11
}
,
12
13
// here you would implement the guardrail logic for streaming
14
// Note: streaming guardrails are difficult to implement, because
15
// you do not know the full content of the stream until it's finished.
16
}
;
Configuring Per Request Custom Metadata
To send and access custom metadata in Middleware, you can use
providerOptions
. This is useful when building logging middleware where you want to pass additional context like user IDs, timestamps, or other contextual data that can help with tracking and debugging.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
,
wrapLanguageModel
}
from
'ai'
;
2
import
type
{
LanguageModelV3Middleware
}
from
'@ai-sdk/provider'
;
3
4
export
const
yourLogMiddleware
:
LanguageModelV3Middleware
=
{
5
wrapGenerate
:
async
(
{
doGenerate
,
params
}
)
=>
{
6
console
.
log
(
'METADATA'
,
params
?.
providerMetadata
?.
yourLogMiddleware
)
;
7
const
result
=
await
doGenerate
(
)
;
8
return
result
;
9
}
,
10
}
;
11
12
const
{
text
}
=
await
generateText
(
{
13
model
:
wrapLanguageModel
(
{
14
model
:
"anthropic/claude-sonnet-4.5"
,
15
middleware
:
yourLogMiddleware
,
16
}
)
,
17
prompt
:
'Invent a new holiday and describe its traditions.'
,
18
providerOptions
:
{
19
yourLogMiddleware
:
{
20
hello
:
'world'
,
21
}
,
22
}
,
23
}
)
;
24
25
console
.
log
(
text
)
;
Previous
Speech
Next
Provider & Model Management
On this page
Language Model Middleware
Using Language Model Middleware
Multiple middlewares
Built-in Middleware
Extract Reasoning
Simulate Streaming
Default Settings
Add Tool Input Examples
Options
Community Middleware
Custom tool call parser
Implementing Language Model Middleware
Examples
Logging
Caching
Retrieval Augmented Generation (RAG)
Guardrails
Configuring Per Request Custom Metadata
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== PROVIDER-MANAGEMENT =====

AI SDK Core: Provider & Model Management
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Provider & Model Management
Copy markdown
Provider & Model Management
When you work with multiple providers and models, it is often desirable to manage them in a central place
and access the models through simple string ids.
The AI SDK offers
custom providers
and
a
provider registry
for this purpose:
With
custom providers
, you can pre-configure model settings, provide model name aliases,
and limit the available models.
The
provider registry
lets you mix multiple providers and access them through simple string ids.
You can mix and match custom providers, the provider registry, and
middleware
in your application.
Custom Providers
You can create a
custom provider
using
customProvider
.
Example: custom model settings
You might want to override the default model settings for a provider or provide model name aliases
with pre-configured settings.
1
import
{
2
gateway
,
3
customProvider
,
4
defaultSettingsMiddleware
,
5
wrapLanguageModel
,
6
}
from
'ai'
;
7
8
// custom provider with different provider options:
9
export
const
openai
=
customProvider
(
{
10
languageModels
:
{
11
// replacement model with custom provider options:
12
'gpt-5.1'
:
wrapLanguageModel
(
{
13
model
:
gateway
(
'openai/gpt-5.1'
)
,
14
middleware
:
defaultSettingsMiddleware
(
{
15
settings
:
{
16
providerOptions
:
{
17
openai
:
{
18
reasoningEffort
:
'high'
,
19
}
,
20
}
,
21
}
,
22
}
)
,
23
}
)
,
24
// alias model with custom provider options:
25
'gpt-5.1-high-reasoning'
:
wrapLanguageModel
(
{
26
model
:
gateway
(
'openai/gpt-5.1'
)
,
27
middleware
:
defaultSettingsMiddleware
(
{
28
settings
:
{
29
providerOptions
:
{
30
openai
:
{
31
reasoningEffort
:
'high'
,
32
}
,
33
}
,
34
}
,
35
}
)
,
36
}
)
,
37
}
,
38
fallbackProvider
:
gateway
,
39
}
)
;
Example: model name alias
You can also provide model name aliases, so you can update the model version in one place in the future:
1
import
{
customProvider
,
gateway
}
from
'ai'
;
2
3
// custom provider with alias names:
4
export
const
anthropic
=
customProvider
(
{
5
languageModels
:
{
6
opus
:
gateway
(
'anthropic/claude-opus-4.1'
)
,
7
sonnet
:
gateway
(
'anthropic/claude-sonnet-4.5'
)
,
8
haiku
:
gateway
(
'anthropic/claude-haiku-4.5'
)
,
9
}
,
10
fallbackProvider
:
gateway
,
11
}
)
;
Example: limit available models
You can limit the available models in the system, even if you have multiple providers.
1
import
{
2
customProvider
,
3
defaultSettingsMiddleware
,
4
wrapLanguageModel
,
5
gateway
,
6
}
from
'ai'
;
7
8
export
const
myProvider
=
customProvider
(
{
9
languageModels
:
{
10
'text-medium'
:
gateway
(
'anthropic/claude-3-5-sonnet-20240620'
)
,
11
'text-small'
:
gateway
(
'openai/gpt-5-mini'
)
,
12
'reasoning-medium'
:
wrapLanguageModel
(
{
13
model
:
gateway
(
'openai/gpt-5.1'
)
,
14
middleware
:
defaultSettingsMiddleware
(
{
15
settings
:
{
16
providerOptions
:
{
17
openai
:
{
18
reasoningEffort
:
'high'
,
19
}
,
20
}
,
21
}
,
22
}
)
,
23
}
)
,
24
'reasoning-fast'
:
wrapLanguageModel
(
{
25
model
:
gateway
(
'openai/gpt-5.1'
)
,
26
middleware
:
defaultSettingsMiddleware
(
{
27
settings
:
{
28
providerOptions
:
{
29
openai
:
{
30
reasoningEffort
:
'low'
,
31
}
,
32
}
,
33
}
,
34
}
)
,
35
}
)
,
36
}
,
37
embeddingModels
:
{
38
embedding
:
gateway
.
embeddingModel
(
'openai/text-embedding-3-small'
)
,
39
}
,
40
// no fallback provider
41
}
)
;
Provider Registry
You can create a
provider registry
with multiple providers and models using
createProviderRegistry
.
Setup
registry.ts
1
import
{
anthropic
}
from
'@ai-sdk/anthropic'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
import
{
createProviderRegistry
,
gateway
}
from
'ai'
;
4
5
export
const
registry
=
createProviderRegistry
(
{
6
// register provider with prefix and default setup using gateway:
7
gateway
,
8
9
// register provider with prefix and direct provider import:
10
anthropic
,
11
openai
,
12
}
)
;
Setup with Custom Separator
By default, the registry uses
:
as the separator between provider and model IDs. You can customize this separator:
registry.ts
1
import
{
anthropic
}
from
'@ai-sdk/anthropic'
;
2
import
{
openai
}
from
'@ai-sdk/openai'
;
3
import
{
createProviderRegistry
,
gateway
}
from
'ai'
;
4
5
export
const
customSeparatorRegistry
=
createProviderRegistry
(
6
{
7
gateway
,
8
anthropic
,
9
openai
,
10
}
,
11
{
separator
:
' > '
}
,
12
)
;
Example: Use language models
You can access language models by using the
languageModel
method on the registry.
The provider id will become the prefix of the model id:
providerId:modelId
.
1
import
{
generateText
}
from
'ai'
;
2
import
{
registry
}
from
'./registry'
;
3
4
const
{
text
}
=
await
generateText
(
{
5
model
:
registry
.
languageModel
(
'openai:gpt-5.1'
)
,
// default separator
6
// or with custom separator:
7
// model: customSeparatorRegistry.languageModel('openai > gpt-5.1'),
8
prompt
:
'Invent a new holiday and describe its traditions.'
,
9
}
)
;
Example: Use text embedding models
You can access text embedding models by using the
.embeddingModel
method on the registry.
The provider id will become the prefix of the model id:
providerId:modelId
.
1
import
{
embed
}
from
'ai'
;
2
import
{
registry
}
from
'./registry'
;
3
4
const
{
embedding
}
=
await
embed
(
{
5
model
:
registry
.
embeddingModel
(
'openai:text-embedding-3-small'
)
,
6
value
:
'sunny day at the beach'
,
7
}
)
;
Example: Use image models
You can access image models by using the
imageModel
method on the registry.
The provider id will become the prefix of the model id:
providerId:modelId
.
1
import
{
generateImage
}
from
'ai'
;
2
import
{
registry
}
from
'./registry'
;
3
4
const
{
image
}
=
await
generateImage
(
{
5
model
:
registry
.
imageModel
(
'openai:dall-e-3'
)
,
6
prompt
:
'A beautiful sunset over a calm ocean'
,
7
}
)
;
Combining Custom Providers, Provider Registry, and Middleware
The central idea of provider management is to set up a file that contains all the providers and models you want to use.
You may want to pre-configure model settings, provide model name aliases, limit the available models, and more.
Here is an example that implements the following concepts:
pass through gateway with a namespace prefix (here:
gateway > *
)
pass through a full provider with a namespace prefix (here:
xai > *
)
setup an OpenAI-compatible provider with custom api key and base URL (here:
custom > *
)
setup model name aliases (here:
anthropic > fast
,
anthropic > writing
,
anthropic > reasoning
)
pre-configure model settings (here:
anthropic > reasoning
)
validate the provider-specific options (here:
AnthropicProviderOptions
)
use a fallback provider (here:
anthropic > *
)
limit a provider to certain models without a fallback (here:
groq > gemma2-9b-it
,
groq > qwen-qwq-32b
)
define a custom separator for the provider registry (here:
>
)
1
import
{
anthropic
,
AnthropicProviderOptions
}
from
'@ai-sdk/anthropic'
;
2
import
{
createOpenAICompatible
}
from
'@ai-sdk/openai-compatible'
;
3
import
{
xai
}
from
'@ai-sdk/xai'
;
4
import
{
groq
}
from
'@ai-sdk/groq'
;
5
import
{
6
createProviderRegistry
,
7
customProvider
,
8
defaultSettingsMiddleware
,
9
gateway
,
10
wrapLanguageModel
,
11
}
from
'ai'
;
12
13
export
const
registry
=
createProviderRegistry
(
14
{
15
// pass through gateway with a namespace prefix
16
gateway
,
17
18
// pass through full providers with namespace prefixes
19
xai
,
20
21
// access an OpenAI-compatible provider with custom setup
22
custom
:
createOpenAICompatible
(
{
23
name
:
'provider-name'
,
24
apiKey
:
process
.
env
.
CUSTOM_API_KEY
,
25
baseURL
:
'https://api.custom.com/v1'
,
26
}
)
,
27
28
// setup model name aliases
29
anthropic
:
customProvider
(
{
30
languageModels
:
{
31
fast
:
anthropic
(
'claude-haiku-4-5'
)
,
32
33
// simple model
34
writing
:
anthropic
(
'claude-sonnet-4-5'
)
,
35
36
// extended reasoning model configuration:
37
reasoning
:
wrapLanguageModel
(
{
38
model
:
anthropic
(
'claude-sonnet-4-5'
)
,
39
middleware
:
defaultSettingsMiddleware
(
{
40
settings
:
{
41
maxOutputTokens
:
100000
,
// example default setting
42
providerOptions
:
{
43
anthropic
:
{
44
thinking
:
{
45
type
:
'enabled'
,
46
budgetTokens
:
32000
,
47
}
,
48
}
satisfies AnthropicProviderOptions
,
49
}
,
50
}
,
51
}
)
,
52
}
)
,
53
}
,
54
fallbackProvider
:
anthropic
,
55
}
)
,
56
57
// limit a provider to certain models without a fallback
58
groq
:
customProvider
(
{
59
languageModels
:
{
60
'gemma2-9b-it'
:
groq
(
'gemma2-9b-it'
)
,
61
'qwen-qwq-32b'
:
groq
(
'qwen-qwq-32b'
)
,
62
}
,
63
}
)
,
64
}
,
65
{
separator
:
' > '
}
,
66
)
;
67
68
// usage:
69
const
model
=
registry
.
languageModel
(
'anthropic > reasoning'
)
;
Global Provider Configuration
The AI SDK 5 includes a global provider feature that allows you to specify a model using just a plain model ID string:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
}
from
'ai'
;
2
3
const
result
=
await
streamText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
// Uses the global provider (defaults to gateway)
5
prompt
:
'Invent a new holiday and describe its traditions.'
,
6
}
)
;
By default, the global provider is set to the Vercel AI Gateway.
Customizing the Global Provider
You can set your own preferred global provider:
setup.ts
1
import
{
openai
}
from
'@ai-sdk/openai'
;
2
3
// Initialize once during startup:
4
globalThis
.
AI_SDK_DEFAULT_PROVIDER
=
openai
;
app.ts
1
import
{
streamText
}
from
'ai'
;
2
3
const
result
=
await
streamText
(
{
4
model
:
'gpt-5.1'
,
// Uses OpenAI provider without prefix
5
prompt
:
'Invent a new holiday and describe its traditions.'
,
6
}
)
;
This simplifies provider usage and makes it easier to switch between providers without changing your model references throughout your codebase.
Previous
Language Model Middleware
Next
Error Handling
On this page
Provider & Model Management
Custom Providers
Example: custom model settings
Example: model name alias
Example: limit available models
Provider Registry
Setup
Setup with Custom Separator
Example: Use language models
Example: Use text embedding models
Example: Use image models
Combining Custom Providers, Provider Registry, and Middleware
Global Provider Configuration
Customizing the Global Provider
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== ERROR-HANDLING =====

AI SDK Core: Error Handling
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Error Handling
Copy markdown
Error Handling
Handling regular errors
Regular errors are thrown and can be handled using the
try/catch
block.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
generateText
}
from
'ai'
;
2
3
try
{
4
const
{
text
}
=
await
generateText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
prompt
:
'Write a vegetarian lasagna recipe for 4 people.'
,
7
}
)
;
8
}
catch
(
error
)
{
9
// handle error
10
}
See
Error Types
for more information on the different types of errors that may be thrown.
Handling streaming errors (simple streams)
When errors occur during streams that do not support error chunks,
the error is thrown as a regular error.
You can handle these errors using the
try/catch
block.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
}
from
'ai'
;
2
3
try
{
4
const
{
textStream
}
=
streamText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
prompt
:
'Write a vegetarian lasagna recipe for 4 people.'
,
7
}
)
;
8
9
for
await
(
const
textPart
of
textStream
)
{
10
process
.
stdout
.
write
(
textPart
)
;
11
}
12
}
catch
(
error
)
{
13
// handle error
14
}
Handling streaming errors (streaming with
error
support)
Full streams support error parts.
You can handle those parts similar to other parts.
It is recommended to also add a try-catch block for errors that
happen outside of the streaming.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
}
from
'ai'
;
2
3
try
{
4
const
{
fullStream
}
=
streamText
(
{
5
model
:
"anthropic/claude-sonnet-4.5"
,
6
prompt
:
'Write a vegetarian lasagna recipe for 4 people.'
,
7
}
)
;
8
9
for
await
(
const
part
of
fullStream
)
{
10
switch
(
part
.
type
)
{
11
// ... handle other part types
12
13
case
'error'
:
{
14
const
error
=
part
.
error
;
15
// handle error
16
break
;
17
}
18
19
case
'abort'
:
{
20
// handle stream abort
21
break
;
22
}
23
24
case
'tool-error'
:
{
25
const
error
=
part
.
error
;
26
// handle error
27
break
;
28
}
29
}
30
}
31
}
catch
(
error
)
{
32
// handle error
33
}
Handling stream aborts
When streams are aborted (e.g., via chat stop button), you may want to perform cleanup operations like updating stored messages in your UI. Use the
onAbort
callback to handle these cases.
The
onAbort
callback is called when a stream is aborted via
AbortSignal
, but
onFinish
is not called. This ensures you can still update your UI state appropriately.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
}
from
'ai'
;
2
3
const
{
textStream
}
=
streamText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
prompt
:
'Write a vegetarian lasagna recipe for 4 people.'
,
6
onAbort
:
(
{
steps
}
)
=>
{
7
// Update stored messages or perform cleanup
8
console
.
log
(
'Stream aborted after'
,
steps
.
length
,
'steps'
)
;
9
}
,
10
onFinish
:
(
{
steps
,
totalUsage
}
)
=>
{
11
// This is called on normal completion
12
console
.
log
(
'Stream completed normally'
)
;
13
}
,
14
}
)
;
15
16
for
await
(
const
textPart
of
textStream
)
{
17
process
.
stdout
.
write
(
textPart
)
;
18
}
The
onAbort
callback receives:
steps
: An array of all completed steps before the abort
You can also handle abort events directly in the stream:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
import
{
streamText
}
from
'ai'
;
2
3
const
{
fullStream
}
=
streamText
(
{
4
model
:
"anthropic/claude-sonnet-4.5"
,
5
prompt
:
'Write a vegetarian lasagna recipe for 4 people.'
,
6
}
)
;
7
8
for
await
(
const
chunk
of
fullStream
)
{
9
switch
(
chunk
.
type
)
{
10
case
'abort'
:
{
11
// Handle abort directly in stream
12
console
.
log
(
'Stream was aborted'
)
;
13
break
;
14
}
15
// ... handle other part types
16
}
17
}
Previous
Provider & Model Management
Next
Testing
On this page
Error Handling
Handling regular errors
Handling streaming errors (simple streams)
Handling streaming errors (streaming with error support)
Handling stream aborts
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== TESTING =====

AI SDK Core: Testing
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Testing
Copy markdown
Testing
Testing language models can be challenging, because they are non-deterministic
and calling them is slow and expensive.
To enable you to unit test your code that uses the AI SDK, the AI SDK Core
includes mock providers and test helpers. You can import the following helpers from
ai/test
:
MockEmbeddingModelV3
: A mock embedding model using the
embedding model v3 specification
.
MockLanguageModelV3
: A mock language model using the
language model v3 specification
.
mockId
: Provides an incrementing integer ID.
mockValues
: Iterates over an array of values with each call. Returns the last value when the array is exhausted.
simulateReadableStream
: Simulates a readable stream with delays.
With mock providers and test helpers, you can control the output of the AI SDK
and test your code in a repeatable and deterministic way without actually calling
a language model provider.
Examples
You can use the test helpers with the AI Core functions in your unit tests:
generateText
1
import
{
generateText
}
from
'ai'
;
2
import
{
MockLanguageModelV3
}
from
'ai/test'
;
3
4
const
result
=
await
generateText
(
{
5
model
:
new
MockLanguageModelV3
(
{
6
doGenerate
:
async
(
)
=>
(
{
7
content
:
[
{
type
:
'text'
,
text
:
`
Hello, world!
`
}
]
,
8
finishReason
:
{
unified
:
'stop'
,
raw
:
undefined
}
,
9
usage
:
{
10
inputTokens
:
{
11
total
:
10
,
12
noCache
:
10
,
13
cacheRead
:
undefined
,
14
cacheWrite
:
undefined
,
15
}
,
16
outputTokens
:
{
17
total
:
20
,
18
text
:
20
,
19
reasoning
:
undefined
,
20
}
,
21
}
,
22
warnings
:
[
]
,
23
}
)
,
24
}
)
,
25
prompt
:
'Hello, test!'
,
26
}
)
;
streamText
1
import
{
streamText
,
simulateReadableStream
}
from
'ai'
;
2
import
{
MockLanguageModelV3
}
from
'ai/test'
;
3
4
const
result
=
streamText
(
{
5
model
:
new
MockLanguageModelV3
(
{
6
doStream
:
async
(
)
=>
(
{
7
stream
:
simulateReadableStream
(
{
8
chunks
:
[
9
{
type
:
'text-start'
,
id
:
'text-1'
}
,
10
{
type
:
'text-delta'
,
id
:
'text-1'
,
delta
:
'Hello'
}
,
11
{
type
:
'text-delta'
,
id
:
'text-1'
,
delta
:
', '
}
,
12
{
type
:
'text-delta'
,
id
:
'text-1'
,
delta
:
'world!'
}
,
13
{
type
:
'text-end'
,
id
:
'text-1'
}
,
14
{
15
type
:
'finish'
,
16
finishReason
:
{
unified
:
'stop'
,
raw
:
undefined
}
,
17
logprobs
:
undefined
,
18
usage
:
{
19
inputTokens
:
{
20
total
:
3
,
21
noCache
:
3
,
22
cacheRead
:
undefined
,
23
cacheWrite
:
undefined
,
24
}
,
25
outputTokens
:
{
26
total
:
10
,
27
text
:
10
,
28
reasoning
:
undefined
,
29
}
,
30
}
,
31
}
,
32
]
,
33
}
)
,
34
}
)
,
35
}
)
,
36
prompt
:
'Hello, test!'
,
37
}
)
;
generateObject
1
import
{
generateObject
}
from
'ai'
;
2
import
{
MockLanguageModelV3
}
from
'ai/test'
;
3
import
{
z
}
from
'zod'
;
4
5
const
result
=
await
generateObject
(
{
6
model
:
new
MockLanguageModelV3
(
{
7
doGenerate
:
async
(
)
=>
(
{
8
content
:
[
{
type
:
'text'
,
text
:
`
{"content":"Hello, world!"}
`
}
]
,
9
finishReason
:
{
unified
:
'stop'
,
raw
:
undefined
}
,
10
usage
:
{
11
inputTokens
:
{
12
total
:
10
,
13
noCache
:
10
,
14
cacheRead
:
undefined
,
15
cacheWrite
:
undefined
,
16
}
,
17
outputTokens
:
{
18
total
:
20
,
19
text
:
20
,
20
reasoning
:
undefined
,
21
}
,
22
}
,
23
warnings
:
[
]
,
24
}
)
,
25
}
)
,
26
schema
:
z
.
object
(
{
content
:
z
.
string
(
)
}
)
,
27
prompt
:
'Hello, test!'
,
28
}
)
;
streamObject
1
import
{
streamObject
,
simulateReadableStream
}
from
'ai'
;
2
import
{
MockLanguageModelV3
}
from
'ai/test'
;
3
import
{
z
}
from
'zod'
;
4
5
const
result
=
streamObject
(
{
6
model
:
new
MockLanguageModelV3
(
{
7
doStream
:
async
(
)
=>
(
{
8
stream
:
simulateReadableStream
(
{
9
chunks
:
[
10
{
type
:
'text-start'
,
id
:
'text-1'
}
,
11
{
type
:
'text-delta'
,
id
:
'text-1'
,
delta
:
'{ '
}
,
12
{
type
:
'text-delta'
,
id
:
'text-1'
,
delta
:
'"content": '
}
,
13
{
type
:
'text-delta'
,
id
:
'text-1'
,
delta
:
`
"Hello,
`
}
,
14
{
type
:
'text-delta'
,
id
:
'text-1'
,
delta
:
`
world
`
}
,
15
{
type
:
'text-delta'
,
id
:
'text-1'
,
delta
:
`
!"
`
}
,
16
{
type
:
'text-delta'
,
id
:
'text-1'
,
delta
:
' }'
}
,
17
{
type
:
'text-end'
,
id
:
'text-1'
}
,
18
{
19
type
:
'finish'
,
20
finishReason
:
{
unified
:
'stop'
,
raw
:
undefined
}
,
21
logprobs
:
undefined
,
22
usage
:
{
23
inputTokens
:
{
24
total
:
3
,
25
noCache
:
3
,
26
cacheRead
:
undefined
,
27
cacheWrite
:
undefined
,
28
}
,
29
outputTokens
:
{
30
total
:
10
,
31
text
:
10
,
32
reasoning
:
undefined
,
33
}
,
34
}
,
35
}
,
36
]
,
37
}
)
,
38
}
)
,
39
}
)
,
40
schema
:
z
.
object
(
{
content
:
z
.
string
(
)
}
)
,
41
prompt
:
'Hello, test!'
,
42
}
)
;
Simulate UI Message Stream Responses
You can also simulate
UI Message Stream
responses for testing,
debugging, or demonstration purposes.
Here is a Next example:
route.ts
1
import
{
simulateReadableStream
}
from
'ai'
;
2
3
export
async
function
POST
(
req
:
Request
)
{
4
return
new
Response
(
5
simulateReadableStream
(
{
6
initialDelayInMs
:
1000
,
// Delay before the first chunk
7
chunkDelayInMs
:
300
,
// Delay between chunks
8
chunks
:
[
9
`
data: {"type":"start","messageId":"msg-123"}\n\n
`
,
10
`
data: {"type":"text-start","id":"text-1"}\n\n
`
,
11
`
data: {"type":"text-delta","id":"text-1","delta":"This"}\n\n
`
,
12
`
data: {"type":"text-delta","id":"text-1","delta":" is an"}\n\n
`
,
13
`
data: {"type":"text-delta","id":"text-1","delta":" example."}\n\n
`
,
14
`
data: {"type":"text-end","id":"text-1"}\n\n
`
,
15
`
data: {"type":"finish"}\n\n
`
,
16
`
data: [DONE]\n\n
`
,
17
]
,
18
}
)
.
pipeThrough
(
new
TextEncoderStream
(
)
)
,
19
{
20
status
:
200
,
21
headers
:
{
22
'Content-Type'
:
'text/event-stream'
,
23
'Cache-Control'
:
'no-cache'
,
24
Connection
:
'keep-alive'
,
25
'x-vercel-ai-ui-message-stream'
:
'v1'
,
26
}
,
27
}
,
28
)
;
29
}
Previous
Error Handling
Next
Telemetry
On this page
Testing
Examples
generateText
streamText
generateObject
streamObject
Simulate UI Message Stream Responses
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== TELEMETRY =====

AI SDK Core: Telemetry
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
Telemetry
Copy markdown
Telemetry
AI SDK Telemetry is experimental and may change in the future.
The AI SDK uses
OpenTelemetry
to collect telemetry data.
OpenTelemetry is an open-source observability framework designed to provide
standardized instrumentation for collecting telemetry data.
Check out the
AI SDK Observability Integrations
to see providers that offer monitoring and tracing for AI SDK applications.
Enabling telemetry
For Next.js applications, please follow the
Next.js OpenTelemetry guide
to enable telemetry first.
You can then use the
experimental_telemetry
option to enable telemetry on specific function calls while the feature is experimental:
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateText
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
prompt
:
'Write a short story about a cat.'
,
4
experimental_telemetry
:
{
isEnabled
:
true
}
,
5
}
)
;
When telemetry is enabled, you can also control if you want to record the input values and the output values for the function.
By default, both are enabled. You can disable them by setting the
recordInputs
and
recordOutputs
options to
false
.
Disabling the recording of inputs and outputs can be useful for privacy, data transfer, and performance reasons.
You might for example want to disable recording inputs if they contain sensitive information.
Telemetry Metadata
You can provide a
functionId
to identify the function that the telemetry data is for,
and
metadata
to include additional information in the telemetry data.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
result
=
await
generateText
(
{
2
model
:
"anthropic/claude-sonnet-4.5"
,
3
prompt
:
'Write a short story about a cat.'
,
4
experimental_telemetry
:
{
5
isEnabled
:
true
,
6
functionId
:
'my-awesome-function'
,
7
metadata
:
{
8
something
:
'custom'
,
9
someOtherThing
:
'other-value'
,
10
}
,
11
}
,
12
}
)
;
Custom Tracer
You may provide a
tracer
which must return an OpenTelemetry
Tracer
. This is useful in situations where
you want your traces to use a
TracerProvider
other than the one provided by the
@opentelemetry/api
singleton.
Gateway
Provider
Custom
Claude Sonnet 4.5
1
const
tracerProvider
=
new
NodeTracerProvider
(
)
;
2
const
result
=
await
generateText
(
{
3
model
:
"anthropic/claude-sonnet-4.5"
,
4
prompt
:
'Write a short story about a cat.'
,
5
experimental_telemetry
:
{
6
isEnabled
:
true
,
7
tracer
:
tracerProvider
.
getTracer
(
'ai'
)
,
8
}
,
9
}
)
;
Collected Data
generateText function
generateText
records 3 types of spans:
ai.generateText
(span): the full length of the generateText call. It contains 1 or more
ai.generateText.doGenerate
spans.
It contains the
basic LLM span information
and the following attributes:
operation.name
:
ai.generateText
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.generateText"
ai.prompt
: the prompt that was used when calling
generateText
ai.response.text
: the text that was generated
ai.response.toolCalls
: the tool calls that were made as part of the generation (stringified JSON)
ai.response.finishReason
: the reason why the generation finished
ai.settings.maxOutputTokens
: the maximum number of output tokens that were set
ai.generateText.doGenerate
(span): a provider doGenerate call. It can contain
ai.toolCall
spans.
It contains the
call LLM span information
and the following attributes:
operation.name
:
ai.generateText.doGenerate
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.generateText.doGenerate"
ai.prompt.messages
: the messages that were passed into the provider
ai.prompt.tools
: array of stringified tool definitions. The tools can be of type
function
or
provider-defined-client
.
Function tools have a
name
,
description
(optional), and
inputSchema
(JSON schema).
Provider-defined-client tools have a
name
,
id
, and
input
(Record).
ai.prompt.toolChoice
: the stringified tool choice setting (JSON). It has a
type
property
(
auto
,
none
,
required
,
tool
), and if the type is
tool
, a
toolName
property with the specific tool.
ai.response.text
: the text that was generated
ai.response.toolCalls
: the tool calls that were made as part of the generation (stringified JSON)
ai.response.finishReason
: the reason why the generation finished
ai.toolCall
(span): a tool call that is made as part of the generateText call. See
Tool call spans
for more details.
streamText function
streamText
records 3 types of spans and 2 types of events:
ai.streamText
(span): the full length of the streamText call. It contains a
ai.streamText.doStream
span.
It contains the
basic LLM span information
and the following attributes:
operation.name
:
ai.streamText
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.streamText"
ai.prompt
: the prompt that was used when calling
streamText
ai.response.text
: the text that was generated
ai.response.toolCalls
: the tool calls that were made as part of the generation (stringified JSON)
ai.response.finishReason
: the reason why the generation finished
ai.settings.maxOutputTokens
: the maximum number of output tokens that were set
ai.streamText.doStream
(span): a provider doStream call.
This span contains an
ai.stream.firstChunk
event and
ai.toolCall
spans.
It contains the
call LLM span information
and the following attributes:
operation.name
:
ai.streamText.doStream
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.streamText.doStream"
ai.prompt.messages
: the messages that were passed into the provider
ai.prompt.tools
: array of stringified tool definitions. The tools can be of type
function
or
provider-defined-client
.
Function tools have a
name
,
description
(optional), and
inputSchema
(JSON schema).
Provider-defined-client tools have a
name
,
id
, and
input
(Record).
ai.prompt.toolChoice
: the stringified tool choice setting (JSON). It has a
type
property
(
auto
,
none
,
required
,
tool
), and if the type is
tool
, a
toolName
property with the specific tool.
ai.response.text
: the text that was generated
ai.response.toolCalls
: the tool calls that were made as part of the generation (stringified JSON)
ai.response.msToFirstChunk
: the time it took to receive the first chunk in milliseconds
ai.response.msToFinish
: the time it took to receive the finish part of the LLM stream in milliseconds
ai.response.avgCompletionTokensPerSecond
: the average number of completion tokens per second
ai.response.finishReason
: the reason why the generation finished
ai.toolCall
(span): a tool call that is made as part of the generateText call. See
Tool call spans
for more details.
ai.stream.firstChunk
(event): an event that is emitted when the first chunk of the stream is received.
ai.response.msToFirstChunk
: the time it took to receive the first chunk
ai.stream.finish
(event): an event that is emitted when the finish part of the LLM stream is received.
It also records a
ai.stream.firstChunk
event when the first chunk of the stream is received.
generateObject function
generateObject
records 2 types of spans:
ai.generateObject
(span): the full length of the generateObject call. It contains 1 or more
ai.generateObject.doGenerate
spans.
It contains the
basic LLM span information
and the following attributes:
operation.name
:
ai.generateObject
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.generateObject"
ai.prompt
: the prompt that was used when calling
generateObject
ai.schema
: Stringified JSON schema version of the schema that was passed into the
generateObject
function
ai.schema.name
: the name of the schema that was passed into the
generateObject
function
ai.schema.description
: the description of the schema that was passed into the
generateObject
function
ai.response.object
: the object that was generated (stringified JSON)
ai.settings.output
: the output type that was used, e.g.
object
or
no-schema
ai.generateObject.doGenerate
(span): a provider doGenerate call.
It contains the
call LLM span information
and the following attributes:
operation.name
:
ai.generateObject.doGenerate
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.generateObject.doGenerate"
ai.prompt.messages
: the messages that were passed into the provider
ai.response.object
: the object that was generated (stringified JSON)
ai.response.finishReason
: the reason why the generation finished
streamObject function
streamObject
records 2 types of spans and 1 type of event:
ai.streamObject
(span): the full length of the streamObject call. It contains 1 or more
ai.streamObject.doStream
spans.
It contains the
basic LLM span information
and the following attributes:
operation.name
:
ai.streamObject
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.streamObject"
ai.prompt
: the prompt that was used when calling
streamObject
ai.schema
: Stringified JSON schema version of the schema that was passed into the
streamObject
function
ai.schema.name
: the name of the schema that was passed into the
streamObject
function
ai.schema.description
: the description of the schema that was passed into the
streamObject
function
ai.response.object
: the object that was generated (stringified JSON)
ai.settings.output
: the output type that was used, e.g.
object
or
no-schema
ai.streamObject.doStream
(span): a provider doStream call.
This span contains an
ai.stream.firstChunk
event.
It contains the
call LLM span information
and the following attributes:
operation.name
:
ai.streamObject.doStream
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.streamObject.doStream"
ai.prompt.messages
: the messages that were passed into the provider
ai.response.object
: the object that was generated (stringified JSON)
ai.response.msToFirstChunk
: the time it took to receive the first chunk
ai.response.finishReason
: the reason why the generation finished
ai.stream.firstChunk
(event): an event that is emitted when the first chunk of the stream is received.
ai.response.msToFirstChunk
: the time it took to receive the first chunk
embed function
embed
records 2 types of spans:
ai.embed
(span): the full length of the embed call. It contains 1
ai.embed.doEmbed
spans.
It contains the
basic embedding span information
and the following attributes:
operation.name
:
ai.embed
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.embed"
ai.value
: the value that was passed into the
embed
function
ai.embedding
: a JSON-stringified embedding
ai.embed.doEmbed
(span): a provider doEmbed call.
It contains the
basic embedding span information
and the following attributes:
operation.name
:
ai.embed.doEmbed
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.embed.doEmbed"
ai.values
: the values that were passed into the provider (array)
ai.embeddings
: an array of JSON-stringified embeddings
embedMany function
embedMany
records 2 types of spans:
ai.embedMany
(span): the full length of the embedMany call. It contains 1 or more
ai.embedMany.doEmbed
spans.
It contains the
basic embedding span information
and the following attributes:
operation.name
:
ai.embedMany
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.embedMany"
ai.values
: the values that were passed into the
embedMany
function
ai.embeddings
: an array of JSON-stringified embedding
ai.embedMany.doEmbed
(span): a provider doEmbed call.
It contains the
basic embedding span information
and the following attributes:
operation.name
:
ai.embedMany.doEmbed
and the functionId that was set through
telemetry.functionId
ai.operationId
:
"ai.embedMany.doEmbed"
ai.values
: the values that were sent to the provider
ai.embeddings
: an array of JSON-stringified embeddings for each value
Span Details
Basic LLM span information
Many spans that use LLMs (
ai.generateText
,
ai.generateText.doGenerate
,
ai.streamText
,
ai.streamText.doStream
,
ai.generateObject
,
ai.generateObject.doGenerate
,
ai.streamObject
,
ai.streamObject.doStream
) contain the following attributes:
resource.name
: the functionId that was set through
telemetry.functionId
ai.model.id
: the id of the model
ai.model.provider
: the provider of the model
ai.request.headers.*
: the request headers that were passed in through
headers
ai.response.providerMetadata
: provider specific metadata returned with the generation response
ai.settings.maxRetries
: the maximum number of retries that were set
ai.telemetry.functionId
: the functionId that was set through
telemetry.functionId
ai.telemetry.metadata.*
: the metadata that was passed in through
telemetry.metadata
ai.usage.completionTokens
: the number of completion tokens that were used
ai.usage.promptTokens
: the number of prompt tokens that were used
Call LLM span information
Spans that correspond to individual LLM calls (
ai.generateText.doGenerate
,
ai.streamText.doStream
,
ai.generateObject.doGenerate
,
ai.streamObject.doStream
) contain
basic LLM span information
and the following attributes:
ai.response.model
: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.
ai.response.id
: the id of the response. Uses the ID from the provider when available.
ai.response.timestamp
: the timestamp of the response. Uses the timestamp from the provider when available.
Semantic Conventions for GenAI operations
gen_ai.system
: the provider that was used
gen_ai.request.model
: the model that was requested
gen_ai.request.temperature
: the temperature that was set
gen_ai.request.max_tokens
: the maximum number of tokens that were set
gen_ai.request.frequency_penalty
: the frequency penalty that was set
gen_ai.request.presence_penalty
: the presence penalty that was set
gen_ai.request.top_k
: the topK parameter value that was set
gen_ai.request.top_p
: the topP parameter value that was set
gen_ai.request.stop_sequences
: the stop sequences
gen_ai.response.finish_reasons
: the finish reasons that were returned by the provider
gen_ai.response.model
: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.
gen_ai.response.id
: the id of the response. Uses the ID from the provider when available.
gen_ai.usage.input_tokens
: the number of prompt tokens that were used
gen_ai.usage.output_tokens
: the number of completion tokens that were used
Basic embedding span information
Many spans that use embedding models (
ai.embed
,
ai.embed.doEmbed
,
ai.embedMany
,
ai.embedMany.doEmbed
) contain the following attributes:
ai.model.id
: the id of the model
ai.model.provider
: the provider of the model
ai.request.headers.*
: the request headers that were passed in through
headers
ai.settings.maxRetries
: the maximum number of retries that were set
ai.telemetry.functionId
: the functionId that was set through
telemetry.functionId
ai.telemetry.metadata.*
: the metadata that was passed in through
telemetry.metadata
ai.usage.tokens
: the number of tokens that were used
resource.name
: the functionId that was set through
telemetry.functionId
Tool call spans
Tool call spans (
ai.toolCall
) contain the following attributes:
operation.name
:
"ai.toolCall"
ai.operationId
:
"ai.toolCall"
ai.toolCall.name
: the name of the tool
ai.toolCall.id
: the id of the tool call
ai.toolCall.args
: the input parameters of the tool call
ai.toolCall.result
: the output result of the tool call. Only available if the tool call is successful and the result is serializable.
Previous
Testing
Next
DevTools
On this page
Telemetry
Enabling telemetry
Telemetry Metadata
Custom Tracer
Collected Data
generateText function
streamText function
generateObject function
streamObject function
embed function
embedMany function
Span Details
Basic LLM span information
Call LLM span information
Basic embedding span information
Tool call spans
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.


===== DEVTOOLS =====

AI SDK Core: DevTools
Docs
Cookbook
Providers
Tools Registry
Tools
Playground
AI Elements
AI Elements
AI Gateway
Gateway
AI SDK 6 is now available.
Read announcement
Menu
v6 (Latest)
AI SDK 6.x
AI SDK by Vercel
Foundations
Overview
Providers and Models
Prompts
Tools
Streaming
Getting Started
Choosing a Provider
Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
TanStack Start
Agents
Overview
Building Agents
Workflow Patterns
Loop Control
Configuring Call Options
AI SDK Core
Overview
Generating Text
Generating Structured Data
Tool Calling
Model Context Protocol (MCP)
Prompt Engineering
Settings
Embeddings
Reranking
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
DevTools
AI SDK UI
Overview
Chatbot
Chatbot Message Persistence
Chatbot Resume Streams
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC
Advanced
Reference
AI SDK Core
AI SDK UI
AI SDK RSC
Stream Helpers
AI SDK Errors
Migration Guides
Troubleshooting
AI SDK Core
DevTools
Copy markdown
DevTools
AI SDK DevTools is experimental and intended for local development only. Do
not use in production environments.
AI SDK DevTools gives you full visibility over your AI SDK calls with
generateText
,
streamText
, and
ToolLoopAgent
. It helps you debug and inspect LLM requests, responses, tool calls, and multi-step interactions through a web-based UI.
DevTools is composed of two parts:
Middleware
: Captures runs and steps from your AI SDK calls
Viewer
: A web UI to inspect the captured data
Installation
Install the DevTools package:
1
pnpm
add
@ai-sdk/devtools
Requirements
AI SDK v6 beta (
ai@^6.0.0-beta.0
)
Node.js compatible runtime
Using DevTools
Add the middleware
Wrap your language model with the DevTools middleware using
wrapLanguageModel
:
1
import
{
wrapLanguageModel
,
gateway
}
from
'ai'
;
2
import
{
devToolsMiddleware
}
from
'@ai-sdk/devtools'
;
3
4
const
model
=
wrapLanguageModel
(
{
5
model
:
gateway
(
'anthropic/claude-sonnet-4.5'
)
,
6
middleware
:
devToolsMiddleware
(
)
,
7
}
)
;
The wrapped model can be used with any AI SDK Core function:
1
import
{
generateText
}
from
'ai'
;
2
3
const
result
=
await
generateText
(
{
4
model
,
// wrapped model with DevTools
5
prompt
:
'What cities are in the United States?'
,
6
}
)
;
Launch the viewer
Start the DevTools viewer:
1
npx @ai-sdk/devtools
Open
http://localhost:4983
to view your AI SDK interactions.
Captured data
The DevTools middleware captures the following information from your AI SDK calls:
Input parameters and prompts
: View the complete input sent to your LLM
Output content and tool calls
: Inspect generated text and tool invocations
Token usage and timing
: Monitor resource consumption and performance
Raw provider data
: Access complete request and response payloads
Runs and steps
DevTools organizes captured data into runs and steps:
Run
: A complete multi-step AI interaction, grouped by the initial prompt
Step
: A single LLM call within a run (e.g., one
generateText
or
streamText
call)
Multi-step interactions, such as those created by tool calling or agent loops, are grouped together as a single run with multiple steps.
How it works
The DevTools middleware intercepts all
generateText
and
streamText
calls through the
language model middleware
system. Captured data is stored locally in a JSON file (
.devtools/generations.json
) and served through a web UI built with Hono and React.
The middleware automatically adds
.devtools
to your
.gitignore
file.
Verify that
.devtools
is in your
.gitignore
to ensure you don't commit
sensitive AI interaction data to your repository.
Security considerations
DevTools stores all AI interactions locally in plain text files, including:
User prompts and messages
LLM responses
Tool call arguments and results
API request and response data
Only use DevTools in local development environments.
Do not enable DevTools in production or when handling sensitive data.
Previous
Telemetry
Next
AI SDK UI
On this page
DevTools
Installation
Requirements
Using DevTools
Add the middleware
Launch the viewer
Captured data
Runs and steps
How it works
Security considerations
Deploy and Scale AI Apps with Vercel
Deliver AI experiences globally with one push.
Trusted by industry leaders:
OpenAI
Photoroom
Sign Up
Resources
Docs
Cookbook
Providers
Tools Registry
Showcase
GitHub
Discussions
More
Playground
Workflow Dev Kit
Flags SDK
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
©
2026
Vercel, Inc.